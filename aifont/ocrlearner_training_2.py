# AUTOGENERATED! DO NOT EDIT! File to edit: 17_experiment_3_rerun.ipynb (unless otherwise specified).

__all__ = ['load_confusion_matrix', 'get_confusion_matrix', 'compare_confusion_matrices', 'VD_HEIJDEN_PATH',
           'TOWNSEND_PATH', 'DUMP_FOLDER', 'get_font_df', 'get_fontsampler_df', 'get_tfms', 'get_dls', 'build_learner',
           'get_model_accuracy_metrics', 'find_tfms_for_accuracy', 'TensorCategoryDistribution',
           'GetCategoryDistribution', 'CategoryDistibutionBlock', 'find_best_variation_per_category',
           'filter_best_variations', 'merge_best_parameters', 'learn']

# Cell

from .core import *
from .ocrlearner import *
from .fontsampler import *
import cornet
from fastai.data.all import *
from fastai.vision.all import *
from enum import Enum, auto
import gc
import hashlib
from itertools import product
from nbdev.showdoc import *
import matplotlib.pyplot as plt
import os
import pandas as pd
from pandas import DataFrame
import PIL
import random
import re
import scipy.io
import string
import torchvision.transforms.functional as VF
import torchvision.transforms as VT
from typing import Callable, List, Sequence, Tuple

# Cell

VD_HEIJDEN_PATH = Path("data/confusion_matrices/van_der_heijden_et_al_1984.csv")
TOWNSEND_PATH = Path("data/confusion_matrices/townsend_1971_cond_1.csv")
DUMP_FOLDER = Path("results")/"nb14_ocr_learner_training_2"
ensure_path(DUMP_FOLDER)

def load_confusion_matrix(
    path: Union[str, Path] = VD_HEIJDEN_PATH,
    symmetrize=False,
) -> DataFrame:
    """Load a saved empirical confusion matrix and format it a bit. Optionally
       `symmetrize` off-diagonal values."""
    cm = pd.read_csv(path)
    cm.rename(columns={cm.columns[0]: "Stimulus"}, inplace=True)
    cm.set_index("Stimulus", drop=True, inplace=True)
    if symmetrize: cm = (cm + cm.T) / 2
    return cm

def get_confusion_matrix(
    ocr_fn: str,
    n_out = 26,
    arch = cornet_for_ocr,
    splitter = cornet_splitter,
    df = get_sans_serif_df,
    tfms = EMPIRICAL_FINETUNING_TFMS,
    ds_idx = 0,
) -> DataFrame:
    """Compute a confusion matrix from output propabilites, not mere misclassifications."""
    ocr_model = load_ocr_model(file=ocr_fn, arch=arch, n_out=n_out)
    learn = build_ocr_learner(
        model=ocr_model,
        splitter=splitter,
        df=df,
        loss_func=CrossEntropyLossFlat(flatten=False),
        **tfms
        )
    # Force augmentations to also work on the validation dataset, because `RandImgTfms`
    # are only applied to the training dataset by default.
    for o in learn.dls.valid.after_item: o.split_idx = None
    preds,targets = learn.get_preds(ds_idx=ds_idx)
    vocab = learn.dls.vocab[0]
    preds_by_class = [[] for _ in vocab]
    for p,c in zip(preds,targets):
        preds_by_class[c.item()].append(p)
    for i,p in enumerate(preds_by_class):
        preds_by_class[i] = [vocab[i]] + [o.item() for o in torch.stack(p).mean(dim=0)]
    cm = DataFrame(preds_by_class)
    cm.columns = ["Stimulus"] + vocab
    cm.set_index("Stimulus", inplace=True, drop=True)
    return cm

def compare_confusion_matrices(
    a: DataFrame,
    b: DataFrame,
    symmetrize=False,
    non_diagonal=False,
) -> DataFrame:
    """Calculate the Pearson correlation between two confusion matrices. For results
       comparable with traditional studies, set both `symmetrize` and `non_diagonal`
       to `True`."""
    from scipy.stats import pearsonr
    if symmetrize: a,b = (a + a.T) / 2, (b + b.T) / 2
    if non_diagonal:
        a_nd,b_nd = [],[]
        for i in range(len(a)):
            a.iloc[i,i],b.iloc[i,i] = 0,0
            a_i,b_i = a.iloc[i].values.tolist(),b.iloc[i].values.tolist()
            del(a_i[i],b_i[i])
            a_nd.append(a_i); b_nd.append(b_i)
        index = a.index.tolist()
        a,b = DataFrame(a_nd),DataFrame(b_nd)
        a.index,b.index = index,index
    if symmetrize:
        # Remove duplicate entries before calculating total R
        lists = [[],[]]
        for i,o in enumerate([a,b]):
            for j in range(len(o)):
                k = j if non_diagonal else j+1
                lists[i].append(o.values[j,:k].tolist())
        flat_r = pearsonr(*[flatten_list(o) for o in lists])
    else:
        flat_r = pearsonr(*[flatten_list(o.values.tolist()) for o in (a,b)])
    per_cat_r = [("Total",) + flat_r]
    for c in a.index:
        per_cat_r.append((c,) + pearsonr(*[o.loc[c].values.tolist() for o in (a,b)]))
    per_cat_r = pd.DataFrame(per_cat_r)
    per_cat_r.columns = ["Stimulus", "Pearson R", "p"]
    per_cat_r.set_index("Stimulus", drop=True, inplace=True)
    return per_cat_r

# Cell

def get_font_df(
    font_paths: list[Union[str, Path]] = [SYS_FONT_PATH/"Arial.ttf"],
    font_size: float = 0.71,
    size: int = 28,
    vocab: list = VOCAB_UC
) -> DataFrame:
    """Create `DataFrame` by rendering the fonts."""
    data = []
    font_size = round(font_size * size)
    # Calculate a nice y position if we have an all-uppercase vocab
    y = font_size + (size - font_size) // 2 if ''.join(vocab).isupper() else None
    for font in font_paths:
        for i,l in enumerate(vocab):
            img = render_text(font, text=l, text_size=size, image_width=size, image_height=size, y=y, as_normalised_array=True)
            data.append([i] + list(255 * (1. - img.flatten())))
    df = pd.DataFrame(data).astype("uint8")
    df.columns = ["Letter_idx"] + [f"{i}_{j}" for i in range(size) for j in range(size)]
    return df

@delegates(get_font_df)
def get_fontsampler_df(
    variants: list[str] = None,
    subsets: list[str] = None,
    category: str = None,
    **kwargs
) -> DataFrame:
    """Create a `DataFrame` by rendering fonts from `FontSampler`."""
    assert any([variants, subsets, category])
    return get_font_df(font_paths=FontSampler(variants=variants, subsets=subsets, category=category).paths, **kwargs)

# Cell

def get_tfms(
    orig_sz:int,
    size=28,
    normalize=False,
    tfms_p=.5,
    use_affine_tfms=True,
    use_xtra_tfms=False,
    blur_size:Union[int,Sequence[int]]=5,
    blur_sigma:Union[int,Sequence[int]]=(.1, 5.),
    noise_f:Tuple[float,float]=(0., .6),
    translate_and_pad=0.,
) -> Tuple[list, list]:
    "Build `tfms` and `item_tfms` for use with `ImageDataLoadersDF.from_df`."
    tfms,item_tfms = [],[]
    if orig_sz < size:
        pad = (size - orig_sz) // 2
        assert orig_sz + 2 * pad == size
        item_tfms += [Pad(pad, fill=255)]
    if normalize: tfms += [get_imagenet_norm()] # ToRGB(),
    if use_xtra_tfms:
        max_rotate = 15.0
        max_warp = .25
        if translate_and_pad > 0:
            item_tfms += [TranslateAndPad(p=tfms_p, max_x=translate_and_pad, max_y=translate_and_pad)]
        if blur_sigma or blur_size:
            size_args = dict(random_size=blur_size) if is_listy(blur_size) else dict(kernel_size=(blur_size,)*2)
            item_tfms += [GaussianBlur(p=tfms_p, sigma=blur_sigma, **size_args)]
        if noise_f:
            item_tfms += [Noise(p=tfms_p, f=noise_f)]
    else:
        max_rotate = 5.0
        max_warp = .1
    if use_affine_tfms:
        aug_tfms = aug_transforms(mult=1.0, do_flip=False, flip_vert=False, max_rotate=max_rotate,
                    min_zoom=0.85, max_zoom=1.15, max_warp=max_warp, p_affine=tfms_p,
                    p_lighting=0., xtra_tfms=None, size=size, mode='bilinear',
                    pad_mode='reflection', align_corners=True, batch=False,
                    min_scale=1.0)
        del(aug_tfms[1]) # Remove lighting tfm
        tfms += aug_tfms
    return tfms,item_tfms

@delegates(get_tfms)
def get_dls(
    df:Union[Callable, DataFrame],
    bs=128,
    start_col=1,
    vocab=VOCAB_UC,
    y_block=None,
    seed=None,
    valid_pct=0.2,
    **kwargs
) -> DataLoaders:
    "Build `DataLoaders`."
    # Get DataFrame
    data = df() if callable(df) else df
    # Check if we need to pad
    orig_sz = math.isqrt(data.shape[1] - start_col)
    # Build tfms
    tfms,item_tfms = get_tfms(orig_sz=orig_sz, **kwargs)
    # tfms,item_tfms=None,None
    # Creata DataLoaders
    dls = ImageDataLoadersDF.from_df(data, vocab=vocab,
                                     width=orig_sz, height=orig_sz,
                                     num_workers=0, # Needed for Mac
                                     valid_pct=valid_pct, # Only training
                                     batch_tfms=tfms,
                                     item_tfms=item_tfms,
                                     color=False, #normalize,
                                     y_block=y_block,
                                     seed=seed, bs=bs)
    return dls

@delegates(get_dls)
def build_learner(
    model:Module=None,
    splitter:Callable=cornet_splitter,
    arch:Callable=cornet_for_ocr,
    loss_func=CrossEntropyLossFlat(),
    opt_func=Adam,
    init=nn.init.kaiming_normal_,
    lr=defaults.lr,
    cbs=None,
    metrics=accuracy,
    path=None,
    model_dir=MODELS_PATH,
    wd=None,
    wd_bn_bias=False,
    train_bn=True,
    moms=(0.95,0.85,0.95),
    **kwargs
) -> Learner:
    """Create a new OCR learner based on `model` and `splitter` or an architecture
       generated by `arch` and dataset `df` (included in `**kwargs`)."""
    dls = get_dls(**kwargs)
    if model is None:
        n_out = get_c(dls)
        # This is a crappy, but we can specify the splitter in arch
        model_and_splitter = arch(n_out, input_shape=(dls.height, dls.width))
        if type(model_and_splitter) is tuple: model,splitter = model_and_splitter
        else: model,splitter = model_and_splitter,lambda m: L(m[0], m[1:]).map(params)
    else: assert splitter is not None
    learn = Learner(dls=dls, model=model, loss_func=loss_func, opt_func=opt_func, lr=lr,
                    splitter=splitter, cbs=cbs,  metrics=metrics, path=path, model_dir=model_dir,
                    wd=wd, wd_bn_bias=wd_bn_bias, train_bn=train_bn, moms=moms)
    return learn

# Cell

@delegates(build_ocr_learner)
def get_model_accuracy_metrics(
    model:Module,
    df:DataFrame,
    iters=1,
    show_batch=True,
    **kwargs
) -> Tuple[Learner, float, dict, float,  dict]:
    """Build a Learner and get accuracy metrics for it. Use `iters` to repeat
       if dataset is small and using random augmentations. The tuple returned
       contains a copy of the `Learner` used, a mean of the propabilites for
       the correct class, a dict of these for each category, basic accurary,
       and basic accuracies for each class."""
    learn = build_ocr_learner(model, df=df, valid_pct=0., **kwargs)
    if show_batch: learn.dls.train.show_batch()
    preds_l,trgts_l = [],[]
    with learn.no_logging():
        for _ in range(iters):
            preds,trgts = (cast(o, Tensor) for o in learn.get_preds(ds_idx=0))
            preds_l.append(preds); trgts_l.append(trgts)
    preds,trgts = concat_tensors(preds_l),concat_tensors(trgts_l)
    trgt_preds = preds.gather(1, trgts.unsqueeze(1)).squeeze()
    vocab = learn.dls.vocab[0]
    prob_dict = {}
    acc_dict = {}
    for i,l in enumerate(vocab):
        mask = (trgts == i).nonzero().squeeze()
        prob_dict[l] = trgt_preds.gather(0, mask).mean().item()
        acc_dict[l]  = accuracy(preds.index_select(0, mask), torch.full_like(mask, i)).item()
    prob_mean = trgt_preds.mean().item()
    acc_mean = accuracy(preds, trgts)
    return learn,prob_mean,prob_dict,acc_mean,acc_dict

@delegates(get_model_accuracy_metrics)
def find_tfms_for_accuracy(
    model:Module,
    df:DataFrame,
    target_acc=.5,
    target_prob=None,
    eps=.01,
    max_iters=40,
    param="blur_size", # or "blur_sigma"
    param_start=5,
    param_step=2,
    param_step_factor:float=0.5,
    base_tfms:dict=None,
    **kwargs
) -> Tuple[Learner,dict]:
    """Find values for tfms to pass to `get_tfms` that result in a mean accuracy of
       `target_acc` +/- `eps`, or `target_prob` which is the softmaxed probability
       of the target class. Note that by default we do not include affine rotation,
       scaling etc. transformations, nor noise. We only change blur."""
    assert param in ("blur_size", "blur_sigma")
    assert (target_acc is None) is not (target_prob is None)
    if param == "blur_size":
        assert param_start % 2 == 1 and param_step % 2 == 0
        param_step_factor = math.ceil(param_step_factor)
    tfms_args = dict(
        use_affine_tfms=False,
        use_xtra_tfms=True,
        tfms_p=1.,
        blur_size=5,
        blur_sigma=3.,
        noise_f=None,
        size=48,
        normalize=True,
        translate_and_pad=0.
    )
    tfms_args[param] = param_start
    if base_tfms: tfms_args = {**tfms_args, **base_tfms}
    acc_mean = 10.
    step = param_step
    i = 0
    while abs(acc_mean - target_acc) > eps and i < max_iters:
        if i > 0:
            # Param should be raised but it's negative or it should be lowered but it's positive,
            # i.e., on the last iter we went past the target. Thus, reverse the sign of the step
            # and lower its abs value.
            if (acc_mean > target_acc and step < 0) or (acc_mean < target_acc and step > 0):
                if param_step_factor == 1: print("No more param options available."); break
                step *= -1 * param_step_factor
            tfms_args[param] += step
        res = get_model_accuracy_metrics(model, df, **tfms_args, **kwargs)
        acc_mean,acc_dict = res[3:5] if target_prob is None else res[1:3]
        print(f"Iter {i} • Param ({param}): {tfms_args[param]} • Step {step} • Accuracy {acc_mean:.4f}")
        i += 1
    print("Final accuracy per letter")
    for l,a in acc_dict.items(): print(f"{l}: {a:.3f}")
    return res[0],tfms_args # res[0] is the Learner

# Cell

class TensorCategoryDistribution(TensorCategory): pass

TensorBase.register_func(Tensor.__getitem__, TensorCategoryDistribution)

class GetCategoryDistribution(DisplayedTransform):
    """Transform category string to propability distribution from `confusion_matrix`.
       Enforces a sum of one for each input's propabilities if `normalize`."""
    loss_func,order,y_tensors=CrossEntropyLossFlat(),1,{}
    def __init__(self,
        confusion_matrix: DataFrame,
        vocab = None,
        sort = True,
        normalize = True
        ):
        if vocab is not None: vocab = CategoryMap(vocab, sort=sort)
        store_attr()

    def setups(self, dsets):
        if self.vocab is None and dsets is not None: self.vocab = CategoryMap(dsets, sort=self.sort)
        self.c = len(self.vocab)
        for o in self.vocab:
            assert o in self.confusion_matrix.index
            t = tensor(self.confusion_matrix.loc[o])
            self.y_tensors[o] = t / t.sum() if self.normalize else t

    def encodes(self, o):
        try:
            return self.y_tensors[o]
        except KeyError as e:
            raise KeyError(f"Label '{o}' was not included in the training dataset") from e

def CategoryDistibutionBlock(
    confusion_matrix: DataFrame,
    vocab = None,
    sort = True,
    normalize = True
):
    "`TransformBlock` category propability distributions."
    return TransformBlock(type_tfms=GetCategoryDistribution(confusion_matrix=confusion_matrix,
                          vocab=vocab, sort=sort, normalize=normalize))

# Cell

from .core import *
from .ocrlearner import *
from .fontsampler import *
from fastai.data.all import *
from fastai.vision.all import *
from enum import Enum, auto
import gc
from nbdev.showdoc import *
import matplotlib.pyplot as plt
# import os
import pandas as pd
from pandas import DataFrame
# import PIL
# import random
# import re
# import scipy.io
# import string
# import torchvision.transforms.functional as VF
# import torchvision.transforms as VT
from typing import Callable, List, Sequence, Tuple

# Cell

from .core import *
from .ocrlearner import *
from .fontlearner import *
from .fontsampler import *
from fastai.data.all import *
from fastai.vision.all import *
from enum import Enum, auto
import gc
from nbdev.showdoc import *
import matplotlib.pyplot as plt
# import os
import pandas as pd
from pandas import DataFrame
# import PIL
# import random
# import re
# import scipy.io
# import string
# import torchvision.transforms.functional as VF
# import torchvision.transforms as VT
from typing import Callable, List, Sequence, Tuple

# Cell

def find_best_variation_per_category(
    vars: list[dict],
    cat_loss_col="per_category_loss",
) -> dict:
    """Find the best variations for each letter category."""
    losses = {k:[] for k in vars[0][cat_loss_col].keys()}
    for o in vars:
        for k,v in o[cat_loss_col].items(): losses[k].append(v)
    mins = {k:min(v)  for k,v in losses.items()}
    out = {}
    for k,v in mins.items():
        for o in vars:
            if o[cat_loss_col][k] == v:
                out[k] = o; break
    return out

def filter_best_variations(
    compare_to: dict,
    best_vars: dict,
    min_improvement=.1,
    cat_loss_col="per_category_loss",
) -> dict:
    """Filter the dict produced by `find_best_variation_per_category` to only
       include letters where improvement to `compared_to` exceeds `min_improvement`."""
    out = {}
    for k,v in best_vars.items():
        if compare_to[cat_loss_col][k] - v[cat_loss_col][k] >= min_improvement:
            out[k] = v
    return out

def merge_best_parameters(
    vars: list[dict],
    base_idx=0,
    min_improvement=.1,
    vocal=True,
    param_col="final_params",
    cat_loss_col="per_category_loss",
) -> dict:
    """Merge the best parameters for each letter to params of variation at `base_idx`
       and return a param_dict."""
    base = vars[base_idx]
    out = {**base[param_col]}
    best_vars = find_best_variation_per_category(vars, cat_loss_col=cat_loss_col)
    best_vars = filter_best_variations(base, best_vars, min_improvement=min_improvement, cat_loss_col=cat_loss_col)
    # Only select params that do not affect other letters
    affected_params = [o for o in base[param_col].keys() if \
                       param_affects(o, best_vars.keys(), strict=True, incl_general=False)]
    for p in affected_params:
        # Get a mean over the best params for each of the affected letters
        v = mean([best_vars[o][param_col][p] for o in get_param_letters(p)])
        if vocal: print(f"Change '{p}' from {out[p]:.4} => {v:.4}")
        out[p] = v
    return out

# Cell

learn = get_match_font_learner()
clear()
learn.set_param("Height", 1.5)
display(img_from_tensor(learn.loss_func.target_letters[0][0]))
learn.show_renders_and_target()
# image_grid([img_from_tensor(o[0]) for o in learn.loss_func.target_letters])