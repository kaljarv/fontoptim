{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp ocrlearner\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OCR Learner\n",
    "\n",
    "Simple OCR models for use with font optimiser. We make some spefcific adjustments for this case, including:\n",
    "\n",
    "* Using grayscale images and, thus, 1-channel inputs\n",
    "  * Note that this demands other changes as well: remove ImageNet normalisation routines\n",
    "  * default.cmap to binary instead of viridis\n",
    "* Cutting the Resnet 18 model at layer 5\n",
    "* Balancing input categories\n",
    "\n",
    "To do:\n",
    "\n",
    "* Implement oversampling\n",
    "* Implement normalisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "from aifont.core import *\n",
    "from aifont.fontsampler import FontSampler\n",
    "import cornet\n",
    "from fastai.data.all import *\n",
    "from fastai.vision.all import *\n",
    "from enum import Enum, auto\n",
    "import gc\n",
    "import hashlib\n",
    "from itertools import product\n",
    "from nbdev.showdoc import *\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "import PIL\n",
    "import random\n",
    "import re\n",
    "import scipy.io\n",
    "import string\n",
    "import torchvision.transforms.functional as VF\n",
    "import torchvision.transforms as VT\n",
    "from typing import Callable, List, Sequence, Tuple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Sources"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Files and DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "A_Z_HANDW_PATH     = Path(\"data/A_Z_handwritten_data.csv\")\n",
    "DLS_TEST_PATH      = Path(\"data/az_tmnist_for_dls_test.csv\")\n",
    "GOOGLE_FONT_PATH   = Path(\"data/google_fonts/font_images\")\n",
    "MODELS_PATH        = Path(\"models\")\n",
    "NIST_PATH          = Path(\"data/nist_images\")\n",
    "STANFORD_PATH      = Path(\"data/letter.data.gz\") \n",
    "TMNIST_PATH        = Path(\"data/94_character_TMNIST.csv\")\n",
    "\n",
    "def ascii_to_str(ascii: str) -> str:\n",
    "    \"\"\"Decode an ASCII-encoded character.\"\"\"\n",
    "    return bytes.fromhex(ascii).decode()\n",
    "\n",
    "def nist_from_name_func(name: str) -> str:\n",
    "    \"\"\"Use with `ImageDataLoaders.from_name_func` for nist images.\"\"\"\n",
    "    code = name.split(\"_\")[1] # E.g. train_59_01642.png\n",
    "    return ascii_to_str(code)\n",
    "\n",
    "def pad_to_size(\n",
    "    df: DataFrame, \n",
    "    size: Union[int, tuple[int, int]] = None, # w = h or (w,h)\n",
    "    start_col = 1,\n",
    "    pad_with  = 0.\n",
    "    ) -> DataFrame:\n",
    "    \"\"\"Pad the image data in DataFrame `df` with `pad_with` to match `size`.\n",
    "       Note that column names are reset to integers with -1 for the ones added.\"\"\"\n",
    "    if size is None: return df\n",
    "    nw,nh = (size,size) if type(size) is int else size\n",
    "    init_sz = df.shape[1] - start_col\n",
    "    if init_sz == nw * nh: return df\n",
    "    ow = oh = math.isqrt(init_sz)\n",
    "    assert ow * oh == init_sz, \"Input images must be square\"\n",
    "    pw,ph = (nw - ow), (nh - oh)\n",
    "    assert pw >= 0 and ph >= 0, \"Size must be greater than equal to the original\"\n",
    "    assert pw % 2 == 0 and ph % 2 == 0, \"Padding must be symmetrical in both dims\"\n",
    "    ndf = df.copy()\n",
    "    ndf.columns = range(ndf.shape[1])\n",
    "    cols = np.arange(start_col, start_col + init_sz).reshape((oh,ow))\n",
    "    cols = np.pad(cols, ((ph//2,),(pw//2,)), constant_values=-1)\n",
    "    cols = list(range(start_col)) + cols.ravel().tolist()\n",
    "    return ndf.reindex(columns=cols, fill_value=pad_with)\n",
    "\n",
    "def undersample_df(df: DataFrame, label_col_i = 0, undersample_threshold = 2., seed = 42) -> DataFrame:\n",
    "    \"\"\"Balances samples in categories to match the smallest one times `undersample_threshold`.\"\"\"\n",
    "    counts = df.iloc[:, label_col_i].value_counts()\n",
    "    n_min = round(counts.min() * undersample_threshold)\n",
    "    if (np.array(counts) <= n_min).all(): return df\n",
    "    rs = random.getstate()\n",
    "    random.seed(seed)\n",
    "    keep = []\n",
    "    tot = len(df)\n",
    "    for i, c in counts.iteritems():\n",
    "        ii = df.index[df.iloc[:, label_col_i] == i].tolist()\n",
    "        keep.append(random.sample(ii, n_min) if c > n_min else ii)\n",
    "    df = df.filter(items=[i for ii in keep for i in ii], axis=0)\n",
    "    warn(f\"Undersampled to max {n_min} items / category, using {len(df)} items of {tot}.\")\n",
    "    random.setstate(rs)\n",
    "    return df\n",
    "\n",
    "def get_a_z_handw_images(path=A_Z_HANDW_PATH, undersample = True, \n",
    "                         undersample_threshold = 2., seed = 42) -> DataFrame:\n",
    "    \"\"\"Get a collection of NIST and other images at 28x28.\n",
    "       https://www.kaggle.com/datasets/sachinpatel21/az-handwritten-alphabets-in-csv-format\"\"\"\n",
    "    df = pd.read_csv(path)\n",
    "    if undersample: df = undersample_df(df, label_col_i=0, undersample_threshold=undersample_threshold, seed=seed)\n",
    "    return df\n",
    "\n",
    "def get_tmnist_images(path=TMNIST_PATH, relabel_vocab=VOCAB_UC, undersample = True, \n",
    "                      undersample_threshold = 2., seed = 42, drop_extra_cols = True) -> DataFrame:\n",
    "    \"\"\"Get a collection of renderend font glyphs at 28x28 and relabel with `relabel_vocab`\n",
    "       for combination with `get_a_z_handw_images`.\n",
    "       https://www.kaggle.com/datasets/nikbearbrown/tmnist-alphabet-94-characters\"\"\"\n",
    "    df = pd.read_csv(path)\n",
    "    def get_labels_num(label_str):\n",
    "        label_str = str(label_str)\n",
    "        return relabel_vocab.index(label_str) if label_str in relabel_vocab else np.nan\n",
    "    labels_num = DataFrame([get_labels_num(x) for x in df.labels], columns=['labels_num'])\n",
    "    df = pd.concat([labels_num, df], axis=1)\n",
    "    df.drop(df[df.labels_num.isna()].index, inplace=True)\n",
    "    if drop_extra_cols: df.drop(['names', 'labels'], axis=1, inplace=True)\n",
    "    df.labels_num = df.labels_num.astype(int)\n",
    "    if undersample: df = undersample_df(df, label_col_i=0, undersample_threshold=undersample_threshold, seed=seed)\n",
    "    return df\n",
    "\n",
    "def get_combined_az_and_tmnist_df(undersample = True, undersample_threshold = 2., \n",
    "                                  seed = 42) -> DataFrame:\n",
    "    \"\"\"Combined both the handwritten `get_a_z_handw_images` and `get_tmnist_images`.\n",
    "       NB. These are not weighted much, just a bit for the us_th for tmnist.\"\"\"\n",
    "    df_az  = get_a_z_handw_images(undersample=undersample, undersample_threshold=undersample_threshold, seed=seed)\n",
    "    df_tmnist = get_tmnist_images(undersample=undersample, undersample_threshold=undersample_threshold * .75, seed=seed)\n",
    "    df_tmnist.columns = df_az.columns\n",
    "    return pd.concat((df_az, df_tmnist))\n",
    "\n",
    "def get_az_tmnist_for_dls_test() -> DataFrame:\n",
    "    \"\"\"A truncated dataset that's quick to load when testing `DataLoaders`\"\"\"\n",
    "    df = pd.read_csv(DLS_TEST_PATH)\n",
    "    return df.drop(columns=[df.columns[0]]) # Drop unnamed index\n",
    "    \n",
    "def get_nist_images(vocab: List[str] = None, path = NIST_PATH, show_stats = True, \n",
    "                    undersample = True, undersample_threshold = 2., seed = 42) -> List[Path]:\n",
    "    \"\"\"Get NIST handwritten images optionally filtered by `vocab`. If `undersample`, balances\n",
    "       samples in categories to match the smallest one times `undersample_threshold`.\n",
    "       National Institute of Standards and Technology: NIST Handprinted Forms and Characters\n",
    "       - NIST Special Database 19, 2nd Edition, http://doi.org/10.18434/T4H01C (Accessed Apr 18, 2022)\"\"\"\n",
    "    fnames = {}\n",
    "    folders = os.listdir(path)\n",
    "    for d in folders:\n",
    "        c = ascii_to_str(d)\n",
    "        if vocab is not None and c not in vocab: continue\n",
    "        d_path = os.path.join(path, d)\n",
    "        for d2 in os.listdir(d_path):\n",
    "            if re.search(\"^train\", d2): fnames[c] = get_image_files(os.path.join(d_path, d2))\n",
    "    if show_stats:\n",
    "        cc = sorted(fnames.keys())\n",
    "        nn = [len(fnames[x]) for x in cc]\n",
    "        plt.bar([i for i in range(len(cc))], nn, tick_label=cc)\n",
    "    if undersample:\n",
    "        rs = random.getstate()\n",
    "        random.seed(seed)\n",
    "        nn = [len(x) for x in fnames.values()]\n",
    "        n_min = round(min(nn) * undersample_threshold)\n",
    "        tot = sum(nn)\n",
    "        for x in fnames: \n",
    "            if len(fnames[x]) > n_min: fnames[x] = random.sample(fnames[x], n_min)\n",
    "        warn(f\"Undersampled to max {n_min} items / category, using {sum([len(x) for x in fnames.values()])} items of {tot}.\")\n",
    "        random.setstate(rs)\n",
    "    return [f for ff in fnames.values() for f in ff]\n",
    "\n",
    "def stanford_letters_df(path = STANFORD_PATH) -> DataFrame:\n",
    "    \"\"\"from http://ai.stanford.edu/~btaskar/ocr/\"\"\"\n",
    "    return pd.read_csv(path, delimiter=\"\\t\")\n",
    "\n",
    "def get_font_df(\n",
    "    font_paths: list[Union[str, Path]] = [SYS_FONT_PATH/\"Arial.ttf\"],\n",
    "    font_size: float = 0.71,\n",
    "    font_y: float = None,\n",
    "    size: int = 28,\n",
    "    vocab: list = VOCAB_UC\n",
    ") -> DataFrame:\n",
    "    \"\"\"Create `DataFrame` by rendering the fonts.\"\"\"\n",
    "    data = []\n",
    "    font_size = round(font_size * size)\n",
    "    # Calculate a nice y position if we have an all-uppercase vocab\n",
    "    if font_y is None: font_y = font_size + (size - font_size) // 2 if ''.join(vocab).isupper() else None\n",
    "    else:              font_y = round(font_y * size)\n",
    "    for font in font_paths:\n",
    "        for i,l in enumerate(vocab):\n",
    "            img = render_text(font, text=l, text_size=size, image_width=size, image_height=size, y=font_y, as_normalised_array=True)\n",
    "            data.append([i] + list(255 * (1. - img.flatten())))\n",
    "    df = pd.DataFrame(data).astype(\"uint8\")\n",
    "    df.columns = [\"Letter_idx\"] + [f\"{i}_{j}\" for i in range(size) for j in range(size)]\n",
    "    return df\n",
    "\n",
    "@delegates(get_font_df)\n",
    "def get_fontsampler_df(\n",
    "    variants: list[str] = None, \n",
    "    subsets: list[str] = None, \n",
    "    category: str = None, \n",
    "    **kwargs\n",
    ") -> DataFrame:\n",
    "    \"\"\"Create a `DataFrame` by rendering fonts from `FontSampler`.\"\"\"\n",
    "    assert any([variants, subsets, category])\n",
    "    return get_font_df(font_paths=FontSampler(variants=variants, subsets=subsets, category=category).paths, **kwargs)\n",
    "\n",
    "@delegates(get_fontsampler_df)\n",
    "def get_sans_serif_df(**kwargs) -> DataFrame: \n",
    "    \"\"\"Create a `DataFrame` by rendering Latin sans-serif fonts from `FontSampler`.\"\"\"\n",
    "    return get_fontsampler_df(category=\"sans-serif\", variants=[\"regular\"], subsets=[\"latin\"], **kwargs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Category Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "class TensorCategoryDistribution(TensorCategory): pass\n",
    "\n",
    "TensorBase.register_func(Tensor.__getitem__, TensorCategoryDistribution)\n",
    "\n",
    "class CategoryDistribution(ShowTitle): \n",
    "    \"A simple class for representing category distributions.\"\n",
    "    _show_args = {'label': 'category'}\n",
    "    def __init__(self, category: str, p: float): self.category,self.p = category,p\n",
    "    def __str__(self): return f\"{self.category} ({self.p:0.4f})\"\n",
    "\n",
    "class GetCategoryDistribution(DisplayedTransform):\n",
    "    \"\"\"Transform category string to propability distribution from `confusion_matrix`.\n",
    "       Enforces a sum of one for each input's propabilities if `normalize`.\"\"\"\n",
    "    loss_func,order,y_tensors=CrossEntropyLossFlat(),1,{}\n",
    "    def __init__(self, \n",
    "        confusion_matrix: DataFrame,\n",
    "        vocab = None, \n",
    "        sort = True,\n",
    "        normalize = True\n",
    "        ):\n",
    "        if vocab is not None: vocab = CategoryMap(vocab, sort=sort)\n",
    "        store_attr()\n",
    "\n",
    "    def setups(self, dsets):\n",
    "        if self.vocab is None and dsets is not None: self.vocab = CategoryMap(dsets, sort=self.sort)\n",
    "        self.c = len(self.vocab)\n",
    "        for o in self.vocab:\n",
    "            assert o in self.confusion_matrix.index\n",
    "            t = TensorCategoryDistribution(self.confusion_matrix.loc[o])\n",
    "            self.y_tensors[o] = t / t.sum() if self.normalize else t\n",
    "\n",
    "    def encodes(self, o): \n",
    "        try:\n",
    "            return self.y_tensors[o]\n",
    "        except KeyError as e:\n",
    "            raise KeyError(f\"Label '{o}' was not included in the training dataset\") from e\n",
    "    \n",
    "    def decodes(self, o):\n",
    "        idx = o.argmax().item()\n",
    "        return CategoryDistribution(self.vocab[idx], o[idx].item())\n",
    "\n",
    "def CategoryDistibutionBlock(\n",
    "    confusion_matrix: DataFrame,\n",
    "    vocab = None, \n",
    "    sort = True,\n",
    "    normalize = True\n",
    "):\n",
    "    \"`TransformBlock` category propability distributions.\"\n",
    "    return TransformBlock(type_tfms=GetCategoryDistribution(confusion_matrix=confusion_matrix, \n",
    "                          vocab=vocab, sort=sort, normalize=normalize))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataFrame DataLoader and Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "class ImageReaderDF(DisplayedTransform):\n",
    "    def __init__(self, \n",
    "        start_col = 1, \n",
    "        end_col: int = None, \n",
    "        height = 28, \n",
    "        width = 28,\n",
    "        invert = True, \n",
    "        dtype = 'uint8'\n",
    "        ):\n",
    "        store_attr()\n",
    "\n",
    "    def __call__(self, o, **kwargs):\n",
    "        o = o[self.start_col : self.end_col if self.end_col is not None else len(o)] \\\n",
    "            .values.reshape(self.height, self.width).astype(self.dtype)\n",
    "        return 255 - o if self.invert else o\n",
    "\n",
    "class AlphabetClassReaderDF(DisplayedTransform):\n",
    "    def __init__(self, \n",
    "        label_col = 0, # The column index in the df that holds the label or its index, if vocab is supplied\n",
    "        vocab: List[str] = None, # The vocab\n",
    "        ):\n",
    "        store_attr()\n",
    "\n",
    "    def __call__(self, o, **kwargs):\n",
    "        l = o[self.label_col]\n",
    "        return self.vocab[int(l)] if self.vocab else l\n",
    "\n",
    "class ImageDataLoadersDF(ImageDataLoaders):\n",
    "    @classmethod\n",
    "    @delegates(DataLoaders.from_dblock)\n",
    "    def from_df(cls, df, \n",
    "        valid_pct=0.2, \n",
    "        seed=None, \n",
    "        path=\".\",\n",
    "        start_col=1, \n",
    "        end_col: int=None, \n",
    "        height=28, \n",
    "        width=28,\n",
    "        label_col=0, \n",
    "        vocab: List[str]=None,\n",
    "        y_block=None, \n",
    "        valid_col=None, \n",
    "        item_tfms=None, \n",
    "        batch_tfms=None, \n",
    "        color=False,\n",
    "        **kwargs\n",
    "        ):\n",
    "        if y_block is None: y_block = CategoryBlock\n",
    "        splitter = RandomSplitter(valid_pct, seed=seed) if valid_col is None else ColSplitter(valid_col)\n",
    "        dblock = DataBlock(blocks=(partial(ImageBlock, cls=PILImage if color else PILImageBW), y_block),\n",
    "                           get_x=ImageReaderDF(start_col=start_col, end_col=end_col, height=height, width=width),\n",
    "                           get_y=AlphabetClassReaderDF(label_col=label_col, vocab=vocab),\n",
    "                           splitter=splitter,\n",
    "                           item_tfms=item_tfms,\n",
    "                           batch_tfms=batch_tfms)\n",
    "        return cls.from_dblock(dblock, df, path=path, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stats About the Data\n",
    "\n",
    "Some stats on the data sources, which should be taken into account when choosing the rendering parameters for the vector optimiser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "NIST_STATS = {\n",
    "    'top_min': 2,\n",
    "    'top_mean': 41.7465,\n",
    "    'top_max': 58,\n",
    "    'btm_min': 60,\n",
    "    'btm_mean': 84.995,\n",
    "    'btm_max': 127,\n",
    "    'letter_height_min': 13,\n",
    "    'letter_height_mean': 43.2485,\n",
    "    'letter_height_max': 125,\n",
    "    'width': 128,\n",
    "    'height': 128\n",
    "    }\n",
    "\n",
    "AZ_STATS = {\n",
    "    'top_min': 4,\n",
    "    'top_mean': 4.5936,\n",
    "    'top_max': 12,\n",
    "    'btm_min': 15,\n",
    "    'btm_mean': 22.4108,\n",
    "    'btm_max': 27,\n",
    "    'letter_height_min': 3,\n",
    "    'letter_height_mean': 17.8172,\n",
    "    'letter_height_max': 23,\n",
    "    'width': 28,\n",
    "    'height': 28\n",
    "}\n",
    "TMNIST_STATS = {\n",
    "    'top_min': 0,\n",
    "    'top_mean': 5.1545,\n",
    "    'top_max': 29,\n",
    "    'btm_min': -1,\n",
    "    'btm_mean': 23.0986,\n",
    "    'btm_max': 27,\n",
    "    'letter_height_min': -30,\n",
    "    'letter_height_mean': 17.9441,\n",
    "    'letter_height_max': 20,\n",
    "    'width': 28,\n",
    "    'height': 28\n",
    "}\n",
    "\n",
    "def calc_nist_stats(n=10000):\n",
    "    \"\"\"Calc the tops and bottoms of the letters in the images.\"\"\"\n",
    "    fnames = get_nist_images(show_stats=False, undersample=False)\n",
    "    img = PIL.Image.open(fnames[0])\n",
    "    w, h = img.width, img.height\n",
    "    tops, btms, lhs = [], [], []\n",
    "    for f in random.sample(fnames, n):\n",
    "        img = PIL.Image.open(f)\n",
    "        img_a = np.array(img.getdata())[:,0].reshape(h, w, -1)\n",
    "        top, btm = h + 1, -1\n",
    "        for i in range(h):\n",
    "            if (img_a[i] < 255/2).any():\n",
    "                if i < top: top = i\n",
    "                if i > btm: btm = i\n",
    "        tops.append(top)\n",
    "        btms.append(btm)\n",
    "        lhs.append(btm - top)\n",
    "    tops, btms, lhs = np.array(tops), np.array(btms), np.array(lhs)\n",
    "    return {\n",
    "        \"top_min\":  tops.min(),\n",
    "        \"top_mean\": tops.mean(),\n",
    "        \"top_max\":  tops.max(), \n",
    "        \"btm_min\":  btms.min(), \n",
    "        \"btm_mean\": btms.mean(),\n",
    "        \"btm_max\":  btms.max(),\n",
    "        \"letter_height_min\":  lhs.min(),\n",
    "        \"letter_height_mean\": lhs.mean(),\n",
    "        \"letter_height_max\":  lhs.max(),\n",
    "        \"width\":    w,\n",
    "        \"height\":   h,\n",
    "        }\n",
    "\n",
    "def calc_df_stats(n=10000, df=get_a_z_handw_images):\n",
    "    \"\"\"Calc the tops and bottoms of the letters in the images.\"\"\"\n",
    "    df = df()\n",
    "    rows, cols = df.shape\n",
    "    size = int((cols - 1) ** .5) # Assume one label col\n",
    "    tops, btms, lhs = [], [], []\n",
    "    for i in random.sample(list(range(rows)), n):\n",
    "        top, btm = size + 1, -1\n",
    "        for j in range(1, cols): # Skip first label col\n",
    "            if df.iloc[i, j] > 255/2: # NB. Reverse values\n",
    "                row_n = j // size\n",
    "                if row_n < top: top = row_n\n",
    "                if row_n > btm: btm = row_n\n",
    "        tops.append(top)\n",
    "        btms.append(btm)\n",
    "        lhs.append(btm - top)\n",
    "    tops, btms, lhs = np.array(tops), np.array(btms), np.array(lhs)\n",
    "    return {\n",
    "        \"top_min\":  tops.min(),\n",
    "        \"top_mean\": tops.mean(),\n",
    "        \"top_max\":  tops.max(), \n",
    "        \"btm_min\":  btms.min(), \n",
    "        \"btm_mean\": btms.mean(),\n",
    "        \"btm_max\":  btms.max(),\n",
    "        \"letter_height_min\":  lhs.min(),\n",
    "        \"letter_height_mean\": lhs.mean(),\n",
    "        \"letter_height_max\":  lhs.max(),\n",
    "        \"width\":    size,\n",
    "        \"height\":   size,\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "# For use with `get_dls` later. These are the args used for matching\n",
    "# the OCR model's accuracy to that of empirical studies, i.e., 50%\n",
    "EMPIRICAL_FINETUNING_TFMS = dict(\n",
    "    use_affine_tfms=False,\n",
    "    use_xtra_tfms=True, \n",
    "    tfms_p=1.,\n",
    "    blur_size=17,\n",
    "    blur_sigma=4.41, \n",
    "    noise_f=None,\n",
    "    size=48, \n",
    "    normalize=True,\n",
    "    translate_and_pad=.2\n",
    ")\n",
    "\n",
    "# Another version of these tfms trained without noise-causing translate_and_pad\n",
    "EMPIRICAL_FINETUNING_2_TFMS = dict(\n",
    "    use_affine_tfms=False,\n",
    "    use_xtra_tfms=True, \n",
    "    tfms_p=1.,\n",
    "    blur_size=17,\n",
    "    blur_sigma=5.0625, \n",
    "    noise_f=None,\n",
    "    size=48, \n",
    "    normalize=True,\n",
    "    translate_and_pad=0.\n",
    ")\n",
    "\n",
    "# Yet aother version of these tfms with 50% basic accuracy\n",
    "EMPIRICAL_FINETUNING_3_TFMS = dict(\n",
    "    use_affine_tfms=False,\n",
    "    use_xtra_tfms=True, \n",
    "    tfms_p=1.,\n",
    "    blur_size=21,\n",
    "    blur_sigma=5.46875, \n",
    "    noise_f=None,\n",
    "    size=48, \n",
    "    normalize=True,\n",
    "    translate_and_pad=0.\n",
    ")\n",
    "\n",
    "\n",
    "class BlendingMethod(Enum):\n",
    "    \"For use with `add_noise`\"\n",
    "    MIX = auto()\n",
    "    ADD = auto()\n",
    "    SUB = auto()\n",
    "BLENDING_METHODS = [o for o in BlendingMethod]\n",
    "\n",
    "class NoiseType(Enum):\n",
    "    \"For use with `add_noise`\"\n",
    "    UNIFORM = auto()\n",
    "    GAUSSIAN = auto()\n",
    "NOISE_TYPES = [o for o in NoiseType]\n",
    "\n",
    "class PaddingColor(Enum):\n",
    "    \"For use with `apply_gaussian_blur`\"\n",
    "    MAX = auto()\n",
    "    MIN = auto()\n",
    "\n",
    "def add_noise(x: TensorOrImage,\n",
    "    method: BlendingMethod = None,\n",
    "    noise_type: NoiseType = None,\n",
    "    f: Union[float, Tensor, tuple, list] = None, # Fixed value, range as tuple or None for (0., 1.)\n",
    "    clip = (0., 1.)\n",
    "    ) -> Tensor:\n",
    "    \"\"\"Add random noise to the tensor.\"\"\"\n",
    "    assert is_pil(x) or torch.is_floating_point(x)\n",
    "    if method is None:     method = random.choice(BLENDING_METHODS)\n",
    "    if noise_type is None: noise_type = NoiseType.UNIFORM\n",
    "    if f is None:     fac = random.random()\n",
    "    elif is_listy(f): fac = f[0] + random.random() * (f[1] - f[0])\n",
    "    else:             fac = f\n",
    "    assert method in BLENDING_METHODS and noise_type in NOISE_TYPES\n",
    "    assert noise_type != NoiseType.GAUSSIAN or method == BlendingMethod.ADD\n",
    "    noise = torch.rand(x.shape) if noise_type == NoiseType.UNIFORM else .5 * torch.randn(x.shape)\n",
    "    xt = VF.to_tensor(x) if is_pil(x) else x\n",
    "    assert xt.max() <= 1.1, \"Use image tensors in range (0, 1) for add_noise\"\n",
    "    if   method == BlendingMethod.MIX: xt = (1. - fac) * xt + fac * noise\n",
    "    elif method == BlendingMethod.ADD: xt = xt + fac * noise\n",
    "    else:                              xt = xt - fac * noise # BlendingMethod.SUB\n",
    "    if clip is not None: xt = torch.clip(xt, *clip)\n",
    "    return VF.to_pil_image(xt) if is_pil(x) else xt\n",
    "\n",
    "def apply_gaussian_blur(x: TensorOrImage,\n",
    "    gb: VT.GaussianBlur,\n",
    "    pad_before = PaddingColor.MAX\n",
    "    ) -> TensorOrImage:\n",
    "    \"\"\"Apply a `VT.GaussianBlur` with pre-padding to get rid artifacts at the edges.\"\"\"\n",
    "    kernel_size = gb.kernel_size\n",
    "    xt = VF.to_tensor(x) if is_pil(x) else x\n",
    "    if pad_before is not None:\n",
    "        if   pad_before == PaddingColor.MAX: fill = math.ceil(xt.max())\n",
    "        elif pad_before == PaddingColor.MIN: fill = math.floor(xt.min())\n",
    "        else: raise ValueError(f\"Unknown PaddingColor {pad_before}.\")\n",
    "        size = xt.shape[-2:]\n",
    "        xt = VF.pad(xt,\n",
    "                    padding=kernel_size,\n",
    "                    fill=fill,\n",
    "                    padding_mode=\"constant\")\n",
    "    xt = gb(xt)\n",
    "    if pad_before is not None: xt = VF.crop(xt, kernel_size[1], kernel_size[0], *size)\n",
    "    return VF.to_pil_image(xt) if is_pil(x) else xt\n",
    "\n",
    "def rand_translate_and_pad(x: TensorOrImage,\n",
    "    max_x = 0.1,\n",
    "    max_y = 0.1,\n",
    "    min_x = 0.,\n",
    "    min_y = 0.,\n",
    "    apply_to_batch = False\n",
    "    ) -> TensorOrImage:\n",
    "    \"\"\"Translate the image tensor `x` with reflection padding by a random fraction \n",
    "       of the image size. If `apply_to_batch`, apply the same random values to the\n",
    "       whole batch. No that without `apply_to_batch` the performance might be slow.\"\"\"\n",
    "    sz = list(x.shape[-2:])\n",
    "    def _rnd_tl(): return (round((min_x + random.random() * (max_x - min_x)) * sz[0]),\n",
    "                           round((min_y + random.random() * (max_y - min_y)) * sz[1]))\n",
    "    if apply_to_batch or is_pil(x) or x.ndim < 4:  return translate_and_pad(x, *_rnd_tl())\n",
    "    elif x.ndim == 4: return torch.concat([translate_and_pad(o, *_rnd_tl()) for o in x]).reshape((-1, *x.shape[-3:]))\n",
    "    else: raise ValueError(f\"Can't deal with {x.ndim} dimensions unless apply_to_batch is True.\")\n",
    "\n",
    "def translate_and_pad(x: TensorOrImage,\n",
    "    tl_x: int = 0,\n",
    "    tl_y: int = 0\n",
    "    ) -> TensorOrImage:\n",
    "    \"\"\"Translate the image tensor `x` by `tl_x` and `tl_y` with reflection padding.\"\"\"\n",
    "    xt = VF.to_tensor(x) if is_pil(x) else x\n",
    "    sz = list(xt.shape[-2:])\n",
    "    assert sz[0] // 2 >= abs(tl_x) and sz[1] // 2 >= abs(tl_y)\n",
    "    xt = VF.crop(xt,\n",
    "                 top    = max(-tl_y, 0),\n",
    "                 left   = max(-tl_x, 0),\n",
    "                 height = sz[0] - abs(tl_y),\n",
    "                 width  = sz[1] - abs(tl_x))\n",
    "    xt = VF.pad(xt,\n",
    "                padding=[\n",
    "                     tl_x if tl_x > 0 else 0,\n",
    "                     tl_y if tl_y > 0 else 0,\n",
    "                    -tl_x if tl_x < 0 else 0,\n",
    "                    -tl_y if tl_y < 0 else 0,\n",
    "                ],\n",
    "                padding_mode=\"reflect\")\n",
    "    return VF.to_pil_image(xt) if is_pil(x) else xt\n",
    "\n",
    "def get_imagenet_norm() -> Normalize:\n",
    "    \"\"\"Perform ImageNet normalisation\"\"\"\n",
    "    return Normalize.from_stats(mean=[0.485, 0.456, 0.406],\n",
    "                                std=[0.229, 0.224, 0.225])\n",
    "\n",
    "class Pad(DisplayedTransform):\n",
    "    \"\"\"Pad image or tensor. Can't use `CropPad` as it doesn't operate on `ImageTensor`.\n",
    "       `size` is either h=v or (h,v).\"\"\"\n",
    "    def __init__(self, \n",
    "        size: Union[int, Tuple[int, int]],\n",
    "        fill: Union[int, float] = None\n",
    "        ):\n",
    "        super().__init__()\n",
    "        if type(size) is int: size = (size,)*2\n",
    "        store_attr()\n",
    "    def encodes(self, x: TensorCategory): return x\n",
    "    def encodes(self, x: TensorOrImage):\n",
    "        return VF.pad(x, padding=self.size, fill=self.fill, \n",
    "                      padding_mode='constant' if self.fill is not None else 'symmetric')\n",
    "    def encodes(self, x): raise ValueError(f\"Pad cannot be applied to {type(x)}\")\n",
    "\n",
    "class RandImgTransform(RandTransform):\n",
    "    \"\"\"Perform one of `self.tfms` on an image or tensor with propability `p`.\"\"\"\n",
    "    def __init__(self, \n",
    "        p = 0.5, \n",
    "        tfms = [noop],\n",
    "        **kwargs\n",
    "        ):\n",
    "        super().__init__(p=p, **kwargs)\n",
    "        store_attr()\n",
    "    def _encodes(self, x): return self.tfms[0](x) if len(self.tfms) == 1 else random.choice(self.tfms)(x)\n",
    "    def encodes(self, x: Tensor):     return self._encodes(x) if is_image(x) else x\n",
    "    def encodes(self, x: ImageTypes): return self._encodes(x)\n",
    "    def encodes(self, x): raise ValueError(f\"RandImgTransform cannot be applied to {type(x)}\")\n",
    "\n",
    "class GaussianBlur(RandImgTransform):\n",
    "    \"\"\"Gaussian blur with propability `p`, kernel `kernel_size` or `random_size`, and `sigma`.\"\"\"\n",
    "    def __init__(self, \n",
    "        p = 0.5, \n",
    "        kernel_size = (3, 3), \n",
    "        random_size: Union[int, Tuple[int, int]] = None,\n",
    "        sigma = (0.1, 5), \n",
    "        pad_before = PaddingColor.MAX,\n",
    "        **kwargs\n",
    "        ):\n",
    "        if random_size is not None:\n",
    "            if type(random_size) is int: random_size = (1, random_size)\n",
    "            assert is_listy(random_size)\n",
    "            for o in random_size: assert o % 2 == 1\n",
    "            kernel_size = product(range(random_size[0], random_size[1] + 2, 2), repeat=2)\n",
    "        else: kernel_size = [kernel_size]\n",
    "        tfms = []\n",
    "        for o in kernel_size:\n",
    "            if o == (1,1): continue\n",
    "            gb = VT.GaussianBlur(kernel_size=o, sigma=sigma)\n",
    "            tfms.append(partial(apply_gaussian_blur, gb=gb, pad_before=pad_before))\n",
    "        super().__init__(p=p, tfms=tfms, **kwargs)\n",
    "        store_attr()\n",
    "\n",
    "class Noise(RandImgTransform):\n",
    "    \"\"\"Add noise with propability `p`, factor `f` and blending `method`. Leave to `None` to randomise.\"\"\"\n",
    "    def __init__(self, \n",
    "        p = 0.5, \n",
    "        f = None,\n",
    "        method: BlendingMethod = None,\n",
    "        noise_type: NoiseType = None,\n",
    "        clip = (0., 1.),\n",
    "        **kwargs\n",
    "        ):\n",
    "        clip = [float(o) for o in clip]\n",
    "        tfms = [partial(add_noise, method=method, noise_type=noise_type, f=f, clip=clip)]\n",
    "        super().__init__(p=p, tfms=tfms, **kwargs)\n",
    "        store_attr()\n",
    "\n",
    "class TranslateAndPad(RandImgTransform):\n",
    "    \"\"\"Translate with padding with propability `p` and random translation with the range specified as\n",
    "       fractions of image size.\"\"\"\n",
    "    def __init__(self, \n",
    "        p = 0.5, \n",
    "        max_x = 0.1,\n",
    "        max_y = 0.1,\n",
    "        min_x = 0.,\n",
    "        min_y = 0.,\n",
    "        apply_to_batch = False,\n",
    "        **kwargs\n",
    "        ):\n",
    "        assert all([max_x < .5, max_y < .5, min_x <= max_x, min_y <= max_y])\n",
    "        tfms = [partial(rand_translate_and_pad, max_x=max_x, max_y=max_y, min_x=min_x, min_y=min_y, \n",
    "                                                apply_to_batch=apply_to_batch)]\n",
    "        super().__init__(p=p, tfms=tfms, **kwargs)\n",
    "        store_attr()\n",
    "\n",
    "class ToRGB(DisplayedTransform):\n",
    "    \"\"\"Expand to 3 channels\"\"\"\n",
    "    def encodes(self, x: Tensor): return tensor_to_rgb(x)\n",
    "    def encodes(self, x: ImageTypes): return x.convert('RGB')\n",
    "    def encodes(self, x): raise ValueError(f\"ToRGB cannot be applied to {type(x)}\")\n",
    "\n",
    "class DebugTfm(DisplayedTransform):\n",
    "    \"\"\"Debug dataloaders\"\"\"\n",
    "    log = []\n",
    "    def __init__(self, msg = \"\", vocal = True):\n",
    "        super().__init__()\n",
    "        store_attr()\n",
    "    def encodes(self, x):\n",
    "        if self.vocal: print(f\"{self.msg}: {type(x)} • {x.shape} • range: {x.min().item():.5f}-{x.max().item():.5f}\"\n",
    "                             f\" • me(di)an {x.mean().item() if torch.is_floating_point(x) else x.median().item():.5f}\")\n",
    "        self.log.append(x)\n",
    "        return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deprecated: Forcing Fastai Vision Tools to Greyscale\n",
    "\n",
    "This is a very hacky way to force images to grayscale! Note that we also need to disable normalisation at `cnn_learner`.\n",
    "\n",
    "These are only needed for image data loaders that read image files. The dataframe-based ones can handle BW image creation themselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "PIL_BASE_ARGS = {}\n",
    "\n",
    "def enable_global_greyscale():\n",
    "    \"\"\"Force all PILImages to greyscale. This is needed to force Fastai vision tools\n",
    "       to greyscale. To undo, call `disable_global_greyscale`.\"\"\"\n",
    "    warn(\"Enabling global grayscale behaviour for PILImages! To undo, call disable_global_greyscale.\")\n",
    "    PIL_BASE_ARGS[\"_open_args\"],PIL_BASE_ARGS[\"_show_args\"] = PILBase._open_args,PILBase._show_args\n",
    "    PILBase._open_args,PILBase._show_args = PILImageBW._open_args,PILImageBW._show_args\n",
    "\n",
    "def disable_global_greyscale():\n",
    "    \"\"\"Undo forcing all PILImages to greyscale with `enable_global_greyscale`.\"\"\"\n",
    "    if len(PIL_BASE_ARGS): PILBase._open_args,PILBase._show_args = PIL_BASE_ARGS[\"_open_args\"],PIL_BASE_ARGS[\"_show_args\"]\n",
    "\n",
    "# This could also be achieved with the following:\n",
    "\n",
    "# def ImageBlockBW(cls=PILImageBW):\n",
    "#     \"A `TransformBlock` for images of `cls`\"\n",
    "#     return TransformBlock(type_tfms=cls.create, batch_tfms=IntToFloatTensor)\n",
    "\n",
    "# def from_path_func_bw(path, fnames, label_func, valid_pct=0.2, seed=None, item_tfms=None, batch_tfms=None, **kwargs):\n",
    "#     \"Create from list of `fnames` in `path`s with `label_func`\"\n",
    "#     dblock = DataBlock(blocks=(ImageBlockBW, CategoryBlock),\n",
    "#                         splitter=RandomSplitter(valid_pct, seed=seed),\n",
    "#                         get_y=label_func,\n",
    "#                         item_tfms=item_tfms,\n",
    "#                         batch_tfms=batch_tfms)\n",
    "#     return ImageDataLoaders.from_dblock(dblock, fnames, path=path, **kwargs)\n",
    "\n",
    "# ImageDataLoaders.from_path_func = from_path_func_bw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Architectures"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CORnet-Z\n",
    "\n",
    "CORnet for letter recognition. We'll:\n",
    "\n",
    "1. Use a model pretrained on ImageNet\n",
    "2. Replace its head with one for letter categorisation\n",
    "3. Freeze all but the last layer\n",
    "4. Retrain with some augmentation and with letters within the centre 28x28px of a 48x48px input image\n",
    "   1. We might want to compare this with inputs padded to 224x224px, which is the size used in pretraining\n",
    "\n",
    "Ref: Kubilius, J., Schrimpf, M., Nayebi, A., Bear, D., Yamins, D.L.K., DiCarlo, J.J. (2018) CORnet: Modeling the Neural Mechanisms of Core Object Recognition. biorxiv. doi:10.1101/408385"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "class CornetLayer(Enum):\n",
    "    V1 = -5\n",
    "    V2 = -4\n",
    "    V4 = -3\n",
    "    IT = -2\n",
    "    decoder = -1\n",
    "\n",
    "def cornet_for_ocr(n_out: int, split_at: CornetLayer = None, *args, **kwargs) -> Tuple[nn.Sequential, callable]:\n",
    "    \"\"\"Create a CORnet-Z model with outputs set to `n_out`.\"\"\"\n",
    "    model = cornet.cornet_z(pretrained=True, map_location=default_device())\n",
    "    model = list(model.children())[0] # Get the Sequential from inside DataParallel\n",
    "    old_lin = model[-1][2]\n",
    "    new_lin = nn.Linear(old_lin.in_features, n_out)\n",
    "    init_cnn(new_lin)\n",
    "    model[-1][2] = new_lin\n",
    "    splitter = cornet_splitter if split_at is None else partial(cornet_splitter, split_at=split_at)\n",
    "    return model,splitter\n",
    "\n",
    "def cornet_splitter(m, split_at: CornetLayer = CornetLayer.decoder): \n",
    "    \"Splitter for CORnet-Z\" \n",
    "    idx = split_at.value\n",
    "    return L(m[:idx], m[idx]).map(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample models from Kaggle submissions.\n",
    "\n",
    "Recreating Keras models with Fastai."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "def count_params(model: nn.Module) -> int:\n",
    "    \"\"\"Count the number of trainable params in `model`.\"\"\"\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "def count_max_pool_output(input_shape, ks=2, padding=0, stride=None):\n",
    "    if stride is None: stride = ks\n",
    "    return [(x + 2 * padding - (ks - 1) - 1) // stride + 1 for x in input_shape[:2]] + (input_shape[2:] if len(input_shape) > 2 else [])\n",
    "\n",
    "def kaggle_cnn_a(n_out: int, input_shape=(28,28)):\n",
    "    \"\"\"https://www.kaggle.com/code/yairhadad1/cnn-for-handwritten-alphabets\n",
    "       807,162 params\"\"\"\n",
    "    # cls = Sequential()\n",
    "    # cls.add(Conv2D(32, (5, 5), input_shape=(28, 28, 1), activation='relu'))\n",
    "    # cls.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    # cls.add(Dropout(0.3))\n",
    "    # cls.add(Flatten())\n",
    "    # cls.add(Dense(128, activation='relu'))\n",
    "    # cls.add(Dense(len(y.unique()), activation='softmax'))\n",
    "    ndim = 2\n",
    "    filters = [32]\n",
    "    ks = [5]\n",
    "    n_pools = 1\n",
    "    preflat_shape = tuple(input_shape)\n",
    "    for _ in range(n_pools): preflat_shape = count_max_pool_output(preflat_shape)\n",
    "    n_flat = int(np.prod(preflat_shape) * filters[-1])\n",
    "    model = sequential(\n",
    "        ConvLayer(1, filters[0], ks=ks[0], ndim=ndim),\n",
    "        nn.Sequential(\n",
    "            MaxPool(ndim=ndim),\n",
    "            nn.Dropout(0.3),\n",
    "            Flatten(),\n",
    "            nn.Linear(n_flat, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, n_out)\n",
    "        )\n",
    "    )\n",
    "    init_cnn(model)\n",
    "    return model\n",
    "\n",
    "def kaggle_cnn_b(n_out: int, input_shape=(28,28)):\n",
    "    \"\"\"https://www.kaggle.com/code/codhek/cnn-using-keras-using-csv-accuracy-99-82\n",
    "       1,132,250 params\"\"\"\n",
    "    # model.add(Conv2D(64, (5, 5), input_shape=(28, 28, 1), activation='relu', data_format=\"channels_last\", padding=\"same\"))\n",
    "    # model.add(Conv2D(64, (5, 5), input_shape=(28, 28, 1), activation='relu', data_format=\"channels_last\", padding=\"same\"))\n",
    "    # model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    # model.add(Conv2D(128, (3, 3), activation='relu', data_format=\"channels_last\", padding=\"same\"))\n",
    "    # model.add(Conv2D(128, (3, 3), activation='relu', data_format=\"channels_last\", padding=\"same\"))\n",
    "    # model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    # model.add(Dropout(0.2))\n",
    "    # model.add(Flatten())\n",
    "    # model.add(Dense(128, activation='relu'))\n",
    "    # model.add(Dense(num_classes, activation='softmax'))\n",
    "    # NB. We are adding BatchNorm which comes with ConvLayer, use norm_type=None to disable\n",
    "    ndim = 2\n",
    "    filters = [64, 128]\n",
    "    ks = [5, 3]\n",
    "    n_pools = 2\n",
    "    preflat_shape = tuple(input_shape)\n",
    "    for _ in range(n_pools): preflat_shape = count_max_pool_output(preflat_shape)\n",
    "    n_flat = int(np.prod(preflat_shape) * filters[-1])\n",
    "    model = sequential(\n",
    "        nn.Sequential(\n",
    "            nn.Sequential(\n",
    "                ConvLayer(1,          filters[0], ks=ks[0], ndim=ndim),\n",
    "                ConvLayer(filters[0], filters[0], ks=ks[0], ndim=ndim),\n",
    "            ),\n",
    "            MaxPool(ndim=ndim),\n",
    "            nn.Sequential(\n",
    "                ConvLayer(filters[0], filters[1], ks=ks[1], ndim=ndim),\n",
    "                ConvLayer(filters[1], filters[1], ks=ks[1], ndim=ndim),\n",
    "            )\n",
    "        ),\n",
    "        nn.Sequential(\n",
    "            MaxPool(ndim=ndim),\n",
    "            nn.Dropout(0.2),\n",
    "            Flatten(),\n",
    "            nn.Linear(n_flat, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, n_out)\n",
    "        )\n",
    "    )\n",
    "    init_cnn(model)\n",
    "    return model\n",
    "\n",
    "def kaggle_cnn_c(n_out: int, input_shape=(28,28)):\n",
    "    \"\"\"https://www.kaggle.com/code/nohrud/99-9-accuracy-on-alphabet-recognition-by-eda\n",
    "       594,170 params\"\"\"\n",
    "    # model.add(Conv2D(32, (5, 5), input_shape=(28, 28, 1), activation='relu'))\n",
    "    # model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    # model.add(Flatten())\n",
    "    # model.add(Dense(128, activation='relu'))\n",
    "    # model.add(Dense(num_classes, activation='softmax'))\n",
    "    ndim = 2\n",
    "    filters = [32]\n",
    "    ks = [5]\n",
    "    n_pools = 1\n",
    "    preflat_shape = tuple(input_shape)\n",
    "    # There's no padding for the first conv, so dims go down\n",
    "    preflat_shape = [x - (ks[0] - 1) for x in preflat_shape]\n",
    "    for _ in range(n_pools): preflat_shape = count_max_pool_output(preflat_shape)\n",
    "    n_flat = int(np.prod(preflat_shape) * filters[-1])\n",
    "    print(n_flat)\n",
    "    model = sequential(\n",
    "        nn.Sequential(\n",
    "            ConvLayer(1, filters[0], ks=ks[0], ndim=ndim, padding=0),\n",
    "        ),\n",
    "        nn.Sequential(\n",
    "            MaxPool(ndim=ndim),\n",
    "            Flatten(),\n",
    "            nn.Linear(n_flat, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, n_out)\n",
    "        )\n",
    "    )\n",
    "    init_cnn(model)\n",
    "    return model\n",
    "    \n",
    "def simple_xresnet(n_out, **kwargs) -> Tuple[nn.Module, Callable]:\n",
    "    \"\"\"This matches XResNet18 and has 10M+ params, so it's huge...\"\"\"\n",
    "    cut = -4\n",
    "    model = XResNet(ResBlock, expansion=1, layers=[2,2,2,2], p=0.0, c_in=1, n_out=n_out, stem_szs=(32,32,64),\n",
    "                    widen=1.0, ks=3, stride=2)\n",
    "    # Cut into body and head as otherwise at least splitter will raise\n",
    "    model = nn.Sequential(nn.Sequential(*list(model.children())[:cut]),\n",
    "                          nn.Sequential(*list(model.children())[cut:]))\n",
    "    def _xresnet_split(m): return L(m[0][:3], m[0][3:], m[1:]).map(params)\n",
    "    return model, _xresnet_split\n",
    "\n",
    "def kaggle_cnn_a_with_res(n_out: int, input_shape=(28,28)):\n",
    "    \"\"\"The Kaggle CNN A model with the first conv layer replaced by a res block\n",
    "       https://www.kaggle.com/code/yairhadad1/cnn-for-handwritten-alphabets\n",
    "       832,922 params\"\"\"\n",
    "    ndim = 2\n",
    "    filters = [32]\n",
    "    ks = [5]\n",
    "    n_pools = 1\n",
    "    preflat_shape = tuple(input_shape)\n",
    "    for _ in range(n_pools): preflat_shape = count_max_pool_output(preflat_shape)\n",
    "    n_flat = int(np.prod(preflat_shape) * filters[-1])\n",
    "    model = sequential(\n",
    "        ResBlock(expansion=1,\n",
    "                 ni=1,\n",
    "                 nf=filters[0], \n",
    "                 stride=1,\n",
    "                 ks=ks[0], \n",
    "                 ndim=ndim),\n",
    "        nn.Sequential(\n",
    "            MaxPool(ndim=ndim),\n",
    "            nn.Dropout(0.3),\n",
    "            Flatten(),\n",
    "            nn.Linear(n_flat, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, n_out)\n",
    "        )\n",
    "    )\n",
    "    init_cnn(model)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create and load OCR Learners and models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "def list_params(learn: Learner) -> None:\n",
    "    \"\"\"Quickly list param groups and params to see which ones are frozen.\"\"\"\n",
    "    for i,g in enumerate(learn.opt.param_groups):\n",
    "        for j,p in enumerate(g[\"params\"]):\n",
    "            print(f\"Group {i} • Param {j}: {p.shape}, {'grad' if p.requires_grad else 'no grad'}\")\n",
    "\n",
    "def load_ocr_model(\n",
    "    file: str = None, \n",
    "    arch: Callable = None, \n",
    "    df: Callable = None, \n",
    "    vocab: List[str] = VOCAB_UC,\n",
    "    version: str = None,\n",
    "    model_dir: str = MODELS_PATH, \n",
    "    n_out: int = None, \n",
    "    **kwargs\n",
    "    ) -> Module:\n",
    "    \"\"\"Load a trained OCR model for use with `OCRLoss`. Pass either `file` and `arch` or\n",
    "       `arch`, `df`, `version` and `vocab` to construct the filename automatically.\"\"\"\n",
    "    assert None not in (file, arch) or None not in (arch, df, vocab)\n",
    "    if file is None: file = get_learner_filename(arch, df, vocab, version=version, incl_suffix=True)\n",
    "    if n_out is None: n_out = len(vocab)\n",
    "    file = os.path.join(model_dir, file)\n",
    "    report(f\"Loading OCR model: {file}\")\n",
    "    # This is awful, sry!\n",
    "    model = arch(n_out=n_out, **kwargs)\n",
    "    model = model[0] if type(model) is tuple else model\n",
    "    load_model(file, model, opt=False, device=None)\n",
    "    return model\n",
    "\n",
    "def get_learner_filename(arch: Callable, df: Callable, \n",
    "    vocab: List[str] = None, \n",
    "    version: str = None,\n",
    "    incl_suffix = False) -> str:\n",
    "    \"\"\"Create a filaneme for the model for use with saving and loading.\"\"\"\n",
    "    sfx = \".pth\" if incl_suffix else \"\"\n",
    "    vcb = f\"__vocab_{hashlib.md5(''.join(vocab).encode('utf-8')).hexdigest()}\" if vocab else \"\"\n",
    "    ver = \"\" if version in (None, \"\") else f\"_{version}\"\n",
    "    return f\"{arch.__name__}__{df.__name__}{vcb}{ver}{sfx}\"\n",
    "\n",
    "def get_tfms(\n",
    "    orig_sz:int,\n",
    "    size=28,\n",
    "    normalize=False, \n",
    "    tfms_p=.5,\n",
    "    use_affine_tfms=True,\n",
    "    use_xtra_tfms=False,\n",
    "    blur_size:Union[int,Sequence[int]]=5,\n",
    "    blur_sigma:Union[int,Sequence[int]]=(.1, 5.), \n",
    "    noise_f:Tuple[float,float]=(0., .6),\n",
    "    noise_type=None,\n",
    "    noise_method=None,\n",
    "    translate_and_pad=0.,\n",
    "    override_max_rotate=None,\n",
    "    override_max_warp=None,\n",
    ") -> Tuple[list, list]:\n",
    "    \"Build `tfms` and `item_tfms` for use with `ImageDataLoadersDF.from_df`.\"\n",
    "    tfms,item_tfms = [],[]\n",
    "    if orig_sz < size: \n",
    "        pad = (size - orig_sz) // 2\n",
    "        assert orig_sz + 2 * pad == size\n",
    "        item_tfms += [Pad(pad, fill=255)]\n",
    "    if normalize: tfms += [get_imagenet_norm()] # ToRGB(), \n",
    "    if use_xtra_tfms:\n",
    "        max_rotate = 15.0 if override_max_rotate is None else override_max_rotate\n",
    "        max_warp   =  .25 if override_max_warp is None else override_max_warp\n",
    "        if translate_and_pad > 0:\n",
    "            item_tfms += [TranslateAndPad(p=tfms_p, max_x=translate_and_pad, max_y=translate_and_pad)]\n",
    "        if blur_sigma or blur_size:\n",
    "            size_args = dict(random_size=blur_size) if is_listy(blur_size) else dict(kernel_size=(blur_size,)*2)\n",
    "            item_tfms += [GaussianBlur(p=tfms_p, sigma=blur_sigma, **size_args)]\n",
    "        if noise_f:\n",
    "            item_tfms += [Noise(p=tfms_p, f=noise_f, noise_type=noise_type, method=noise_method)]\n",
    "    else: \n",
    "        max_rotate = 5.0 if override_max_rotate is None else override_max_rotate\n",
    "        max_warp   =  .1 if override_max_warp is None else override_max_warp\n",
    "    if use_affine_tfms:\n",
    "        aug_tfms = aug_transforms(mult=1.0, do_flip=False, flip_vert=False, max_rotate=max_rotate, \n",
    "                    min_zoom=0.85, max_zoom=1.15, max_warp=max_warp, p_affine=tfms_p, \n",
    "                    p_lighting=0., xtra_tfms=None, size=size, mode='bilinear', \n",
    "                    pad_mode='reflection', align_corners=True, batch=False, \n",
    "                    min_scale=1.0)\n",
    "        del(aug_tfms[1]) # Remove lighting tfm\n",
    "        tfms += aug_tfms\n",
    "    return tfms,item_tfms\n",
    "\n",
    "@delegates(get_tfms)\n",
    "def get_dls(\n",
    "    df:Union[Callable, DataFrame], \n",
    "    bs=128, \n",
    "    start_col=1,\n",
    "    vocab=VOCAB_UC,\n",
    "    y_block=None, \n",
    "    seed=None,\n",
    "    valid_pct=0.2,\n",
    "    **kwargs\n",
    ") -> DataLoaders:\n",
    "    \"Build `DataLoaders`.\"\n",
    "    # Get DataFrame\n",
    "    data = df() if callable(df) else df\n",
    "    # Check bs doesn't exceed data\n",
    "    max_bs = round(len(data) * valid_pct) if valid_pct > 0 else len(data)\n",
    "    if bs > max_bs: bs = max_bs\n",
    "    # Check if we need to pad\n",
    "    orig_sz = math.isqrt(data.shape[1] - start_col)\n",
    "    # Build tfms\n",
    "    tfms,item_tfms = get_tfms(orig_sz=orig_sz, **kwargs)\n",
    "    # tfms,item_tfms=None,None\n",
    "    # Creata DataLoaders\n",
    "    dls = ImageDataLoadersDF.from_df(data, vocab=vocab,\n",
    "                                     width=orig_sz, height=orig_sz,\n",
    "                                     num_workers=0, # Needed for Mac\n",
    "                                     valid_pct=valid_pct, # Only training\n",
    "                                     batch_tfms=tfms,\n",
    "                                     item_tfms=item_tfms,\n",
    "                                     color=False, #normalize,\n",
    "                                     y_block=y_block,\n",
    "                                     seed=seed, bs=bs)\n",
    "    return dls\n",
    "\n",
    "@delegates(get_dls)\n",
    "def build_ocr_learner(\n",
    "    model:Module=None, \n",
    "    splitter:Callable=cornet_splitter,\n",
    "    arch:Callable=cornet_for_ocr,\n",
    "    loss_func=CrossEntropyLossFlat(), \n",
    "    opt_func=Adam, \n",
    "    init=nn.init.kaiming_normal_,\n",
    "    lr=defaults.lr, \n",
    "    cbs=None, \n",
    "    metrics=accuracy, \n",
    "    path=None, \n",
    "    model_dir=MODELS_PATH, \n",
    "    wd=None, \n",
    "    wd_bn_bias=False, \n",
    "    train_bn=True, \n",
    "    moms=(0.95,0.85,0.95),\n",
    "    **kwargs\n",
    ") -> Learner:\n",
    "    \"\"\"Create a new OCR learner based on `model` and `splitter` or an architecture\n",
    "       generated by `arch` and dataset `df` (included in `**kwargs`).\"\"\"\n",
    "    dls = get_dls(**kwargs)\n",
    "    if model is None:\n",
    "        n_out = get_c(dls)\n",
    "        # This is a crappy, but we can specify the splitter in arch\n",
    "        model_and_splitter = arch(n_out, input_shape=(dls.height, dls.width))\n",
    "        if type(model_and_splitter) is tuple: model,splitter = model_and_splitter\n",
    "        else: model,splitter = model_and_splitter,lambda m: L(m[0], m[1:]).map(params)\n",
    "    else: assert splitter is not None\n",
    "    learn = Learner(dls=dls, model=model, loss_func=loss_func, opt_func=opt_func, lr=lr, \n",
    "                    splitter=splitter, cbs=cbs,  metrics=metrics, path=path, model_dir=model_dir, \n",
    "                    wd=wd, wd_bn_bias=wd_bn_bias, train_bn=train_bn, moms=moms)\n",
    "    return learn"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deprecated factory functions for backward compatibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "def get_ocr_learner_2(\n",
    "    arch=kaggle_cnn_a, \n",
    "    n_out=None, \n",
    "    bs=128, \n",
    "    df=get_a_z_handw_images, \n",
    "    vocab=VOCAB_UC,\n",
    "    version=None,\n",
    "    normalize=False, \n",
    "    load_saved=True, \n",
    "    size=28,\n",
    "    loss_func=CrossEntropyLossFlat(), \n",
    "    opt_func=Adam, \n",
    "    init=nn.init.kaiming_normal_,\n",
    "    lr=defaults.lr, \n",
    "    splitter=None, \n",
    "    use_xtra_tfms=False,\n",
    "    cbs=None, \n",
    "    metrics=accuracy, \n",
    "    path=None, \n",
    "    model_dir=MODELS_PATH, \n",
    "    wd=None, \n",
    "    wd_bn_bias=False, \n",
    "    train_bn=True, \n",
    "    moms=(0.95,0.85,0.95), \n",
    "    **kwargs) -> Learner:\n",
    "    \"\"\"Create a new OCR learner based on architecture `arch` and dataset `df`.\n",
    "       If `pretrained`, the model will be initialised if a suitable save is found.\"\"\"\n",
    "    if use_xtra_tfms:\n",
    "        max_rotate = 15.0\n",
    "        max_warp = .25\n",
    "        blur = GaussianBlur(p=.5, random_size=5)\n",
    "        noise = Noise(p=.5, f=(0., .6))\n",
    "        item_tfms = [blur, noise]\n",
    "    else: \n",
    "        max_rotate = 5.0\n",
    "        max_warp = .1\n",
    "        item_tfms = None\n",
    "    tfms = aug_transforms(mult=1.0, do_flip=False, flip_vert=False, max_rotate=max_rotate, \n",
    "                          min_zoom=0.85, max_zoom=1.15, max_warp=max_warp, p_affine=.5, \n",
    "                          p_lighting=0., xtra_tfms=None, size=size, mode='bilinear', \n",
    "                          pad_mode='reflection', align_corners=True, batch=False, \n",
    "                          min_scale=1.0)\n",
    "    del(tfms[1]) # Remove lighting tfm\n",
    "    warn(\"Need to add custom normalisation scheme and check this doesn't expand channels!\")\n",
    "    # See: Normalize.from_stats(mean,std) https://docs.fast.ai/data.transforms.html#Normalize\n",
    "    dls = ImageDataLoadersDF.from_df(df(), vocab=vocab,\n",
    "                                     width=size, height=size,\n",
    "                                     num_workers=0, # Needed for Mac\n",
    "                                     valid_pct=0.2, \n",
    "                                     batch_tfms=tfms,\n",
    "                                     item_tfms=item_tfms,\n",
    "                                     seed=42, bs=bs)\n",
    "\n",
    "    if n_out is None: n_out = get_c(dls)\n",
    "    # This is a crappy, but we can specify the splitter in arch\n",
    "    model_and_splitter = arch(n_out, input_shape=(size, size))\n",
    "    if type(model_and_splitter) is tuple:\n",
    "        model, splitter = model_and_splitter\n",
    "    else: \n",
    "        model = model_and_splitter\n",
    "        splitter = lambda m: L(m[0], m[1:]).map(params)\n",
    "    learn = Learner(dls=dls, model=model, loss_func=loss_func, opt_func=opt_func, lr=lr, \n",
    "                    splitter=splitter, cbs=cbs,  metrics=metrics, path=path, model_dir=model_dir, \n",
    "                    wd=wd, wd_bn_bias=wd_bn_bias, train_bn=train_bn, moms=moms)\n",
    "    if load_saved: learn.load(get_learner_filename(arch=arch, df=df, vocab=vocab, version=version))\n",
    "    store_attr('arch,normalize,n_out', self=learn, **kwargs)\n",
    "    return learn\n",
    "\n",
    "def get_ocr_learner_3(\n",
    "    arch=kaggle_cnn_a, \n",
    "    n_out=None, \n",
    "    bs=128, \n",
    "    df=get_combined_az_and_tmnist_df, \n",
    "    start_col=1,\n",
    "    vocab=VOCAB_UC,\n",
    "    version=None,\n",
    "    normalize=False, \n",
    "    load_saved=True, \n",
    "    size=28,\n",
    "    loss_func=CrossEntropyLossFlat(), \n",
    "    opt_func=Adam, \n",
    "    init=nn.init.kaiming_normal_,\n",
    "    lr=defaults.lr, \n",
    "    splitter=None, \n",
    "    use_xtra_tfms=False,\n",
    "    tfms_p=.5,\n",
    "    blur_size: Tuple[int, Sequence[int]]=5,\n",
    "    blur_sigma: Tuple[int, Sequence[int]]=(.1, 5.), \n",
    "    cbs=None, \n",
    "    metrics=accuracy, \n",
    "    path=None, \n",
    "    model_dir=MODELS_PATH, \n",
    "    wd=None, \n",
    "    wd_bn_bias=False, \n",
    "    train_bn=True, \n",
    "    moms=(0.95,0.85,0.95),\n",
    "    seed=None,\n",
    "    **kwargs) -> Learner:\n",
    "    \"\"\"Create a new OCR learner based on architecture `arch` and dataset `df`.\n",
    "       If `pretrained`, the model will be initialised if a suitable save is found.\"\"\"\n",
    "    tfms = []\n",
    "    item_tfms = []\n",
    "    # Check if we need to pad\n",
    "    data = df()\n",
    "    orig_sz = math.isqrt(data.shape[1] - start_col)\n",
    "    if orig_sz < size: \n",
    "        pad = (size - orig_sz) // 2\n",
    "        assert orig_sz + 2 * pad == size\n",
    "        item_tfms += [Pad(pad, fill=255)]\n",
    "    if normalize: tfms += [get_imagenet_norm()] # ToRGB(), \n",
    "    if use_xtra_tfms:\n",
    "        max_rotate = 15.0\n",
    "        max_warp = .25\n",
    "        blur = GaussianBlur(p=tfms_p, random_size=blur_size, sigma=blur_sigma)\n",
    "        noise = Noise(p=tfms_p, f=(0., .6))\n",
    "        item_tfms += [blur, noise]\n",
    "    else: \n",
    "        max_rotate = 5.0\n",
    "        max_warp = .1\n",
    "    aug_tfms = aug_transforms(mult=1.0, do_flip=False, flip_vert=False, max_rotate=max_rotate, \n",
    "                          min_zoom=0.85, max_zoom=1.15, max_warp=max_warp, p_affine=tfms_p, \n",
    "                          p_lighting=0., xtra_tfms=None, size=size, mode='bilinear', \n",
    "                          pad_mode='reflection', align_corners=True, batch=False, \n",
    "                          min_scale=1.0)\n",
    "    del(aug_tfms[1]) # Remove lighting tfm\n",
    "\n",
    "    tfms += aug_tfms\n",
    "    dls = ImageDataLoadersDF.from_df(data, vocab=vocab,\n",
    "                                     width=orig_sz, height=orig_sz,\n",
    "                                     num_workers=0, # Needed for Mac\n",
    "                                     valid_pct=0.2, \n",
    "                                     batch_tfms=tfms,\n",
    "                                     item_tfms=item_tfms,\n",
    "                                     color=normalize,\n",
    "                                     seed=seed, bs=bs)\n",
    "    if n_out is None: n_out = get_c(dls)\n",
    "    # This is a crappy, but we can specify the splitter in arch\n",
    "    model_and_splitter = arch(n_out, input_shape=(size, size))\n",
    "    if type(model_and_splitter) is tuple:\n",
    "        model,splitter = model_and_splitter\n",
    "    else: \n",
    "        model = model_and_splitter\n",
    "        splitter = lambda m: L(m[0], m[1:]).map(params)\n",
    "    learn = Learner(dls=dls, model=model, loss_func=loss_func, opt_func=opt_func, lr=lr, \n",
    "                    splitter=splitter, cbs=cbs,  metrics=metrics, path=path, model_dir=model_dir, \n",
    "                    wd=wd, wd_bn_bias=wd_bn_bias, train_bn=train_bn, moms=moms)\n",
    "    if load_saved: learn.load(get_learner_filename(arch=arch, df=df, vocab=vocab, version=version))\n",
    "    store_attr('arch,normalize,n_out', self=learn, **kwargs)\n",
    "    return learn"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calibrating Augmentations for 50% Accuracy\n",
    "\n",
    "See notebook 14: OCR Learner Training 2: Match With Human Empirical Data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "@delegates(build_ocr_learner)\n",
    "def get_model_accuracy_metrics(\n",
    "    model:Module, \n",
    "    df:DataFrame,\n",
    "    iters=1,\n",
    "    show_batch=True,\n",
    "    return_iters=False,\n",
    "    **kwargs\n",
    ") -> Union[Tuple[Learner, float, dict, float,  dict], \n",
    "           Tuple[Learner, float, dict, float,  dict, Tensor, Tensor]]:\n",
    "    \"\"\"Build a Learner and get accuracy metrics for it. Use `iters` to repeat \n",
    "       if dataset is small and using random augmentations. The tuple returned\n",
    "       contains a copy of the `Learner` used, a mean of the propabilites for\n",
    "       the correct class, a dict of these for each category, basic accurary,\n",
    "       and basic accuracies for each class. If `return_iters` is `True`, the\n",
    "       full correct class probabilies and target class `Tensors` are also\n",
    "       returned.\"\"\"\n",
    "    learn = build_ocr_learner(model, df=df, valid_pct=0., **kwargs)\n",
    "    if show_batch: learn.dls.train.show_batch()\n",
    "    preds_l,trgts_l = [],[]\n",
    "    with learn.no_logging():\n",
    "        for _ in range(iters):\n",
    "            preds,trgts = (cast(o, Tensor) for o in learn.get_preds(ds_idx=0))\n",
    "            preds_l.append(preds); trgts_l.append(trgts)\n",
    "    preds,trgts = concat_tensors(preds_l),concat_tensors(trgts_l)\n",
    "    trgt_preds = preds.gather(1, trgts.unsqueeze(1)).squeeze()\n",
    "    vocab = learn.dls.vocab[0]\n",
    "    prob_dict = {}\n",
    "    acc_dict = {}\n",
    "    for i,l in enumerate(vocab):\n",
    "        mask = (trgts == i).nonzero().squeeze()\n",
    "        prob_dict[l] = trgt_preds.gather(0, mask).mean().item()\n",
    "        acc_dict[l]  = accuracy(preds.index_select(0, mask), torch.full_like(mask, i)).item()\n",
    "    prob_mean = trgt_preds.mean().item()\n",
    "    acc_mean = accuracy(preds, trgts)\n",
    "    out = (learn,prob_mean,prob_dict,acc_mean,acc_dict)\n",
    "    return out + (trgt_preds,trgts) if return_iters else out\n",
    "\n",
    "@delegates(get_model_accuracy_metrics)\n",
    "def find_tfms_for_accuracy(\n",
    "    model:Module, \n",
    "    df:DataFrame,\n",
    "    target_acc=.5,\n",
    "    target_prob=None,\n",
    "    eps=.01,\n",
    "    max_iters=40,\n",
    "    param=\"blur_size\",\n",
    "    param_start=5,\n",
    "    param_step=2,\n",
    "    param_step_factor:float=0.5,\n",
    "    base_tfms:dict=None,\n",
    "    **kwargs\n",
    ") -> Tuple[Learner,dict]:\n",
    "    \"\"\"Find values for tfms to pass to `get_tfms` that result in a mean accuracy of\n",
    "       `target_acc` +/- `eps`, or `target_prob` which is the softmaxed probability\n",
    "       of the target class. Note that by default we do not include affine rotation,\n",
    "       scaling etc. transformations, nor noise. We only change blur.\"\"\"\n",
    "    assert param in (\"blur_size\", \"blur_sigma\", \"noise_f\")\n",
    "    assert (target_acc is None) is not (target_prob is None)\n",
    "    if param == \"blur_size\": \n",
    "        assert param_start % 2 == 1 and param_step % 2 == 0\n",
    "        param_step_factor = math.ceil(param_step_factor)\n",
    "    tfms_args = dict(\n",
    "        use_affine_tfms=False,\n",
    "        use_xtra_tfms=True, \n",
    "        tfms_p=1.,\n",
    "        blur_size=5,\n",
    "        blur_sigma=3., \n",
    "        noise_f=None,\n",
    "        noise_type=None,\n",
    "        noise_method=None,\n",
    "        size=48, \n",
    "        normalize=True,\n",
    "        translate_and_pad=0.\n",
    "    )\n",
    "    if base_tfms: tfms_args = {**tfms_args, **base_tfms}\n",
    "    tfms_args[param] = param_start\n",
    "    acc_mean = 1e9\n",
    "    step = param_step\n",
    "    target = target_prob if target_prob is not None else target_acc\n",
    "    i = 0\n",
    "    while abs(acc_mean - target) > eps and i < max_iters:\n",
    "        if i > 0:\n",
    "            # Param should be raised but it's negative or it should be lowered but it's positive,\n",
    "            # i.e., on the last iter we went past the target. Thus, reverse the sign of the step\n",
    "            # and lower its abs value.\n",
    "            if (acc_mean > target and step < 0) or (acc_mean < target and step > 0):\n",
    "                if param_step_factor == 1: print(\"No more param options available.\"); break\n",
    "                step *= -1 * param_step_factor\n",
    "            tfms_args[param] += step                \n",
    "        res = get_model_accuracy_metrics(model, df, show_batch=False, **tfms_args, **kwargs)\n",
    "        acc_mean,acc_dict = res[3:5] if target_prob is None else res[1:3]\n",
    "        print(f\"Iter {i} • Param ({param}): {tfms_args[param]} • Step {step} • Accuracy {acc_mean:.4f}\")\n",
    "        i += 1\n",
    "    print(\"Final accuracy per letter\")\n",
    "    for l,a in acc_dict.items(): print(f\"{l}: {a:.3f}\")\n",
    "    learn = res[0]\n",
    "    learn.dls.show_batch()\n",
    "    return learn,tfms_args"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrices: Comparing and OCR Model to Empirical Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, taking into account the critique offered in van der Heijden et al. (1984), the difference in stimulus fonts in Townsend (1971, typewriter) and Gilmore et al. (1979, dot matrix), the low number of observations in Phillips et al. (1983, 60 per letter), and the low number of test subjects in Loomis (1982, 6 female subjects), we use only the data from van der Heijden et al. (1984).\n",
    "\n",
    "See `data/Letter confusion matrices combined.xlsx` for all of the above and further information.\n",
    "\n",
    "**Note** that in, e.g., van def Heijden's data, the most probably response for the stimulus 'Q' was 'O'! This may be look confusing when using `show_batch` later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "CM_FOLDER_PATH     = Path(\"data\")/\"confusion_matrices\"\n",
    "CM_VD_HEIJDEN_PATH = CM_FOLDER_PATH/\"van_der_heijden_et_al_1984.csv\"\n",
    "TOWNSEND_PATH      = CM_FOLDER_PATH/\"townsend_1971_cond_1.csv\"\n",
    "\n",
    "# To get the other matrices, export as csv from the Excel file, but delete\n",
    "# sum columns and rows. You may also need to convert decimal points.\n",
    "\n",
    "def plot_confusion_matrix(\n",
    "    learn: Learner, \n",
    "    change_loss_func = False\n",
    ") -> None:\n",
    "    \"\"\"Utility for plotting a classical confusion matrix. NB. The other\n",
    "       cm functions deal with ones based on class distributions.\"\"\"\n",
    "    loss = learn.loss_func\n",
    "    if change_loss_func: learn.loss_func = CrossEntropyLossFlat()\n",
    "    interp = ClassificationInterpretation.from_learner(learn)\n",
    "    interp.plot_confusion_matrix(figsize=(12,12), dpi=60)\n",
    "    if change_loss_func: learn.loss_func = loss\n",
    "\n",
    "def get_empirical_confusion_matrix(\n",
    "    path: Union[str, Path] = CM_VD_HEIJDEN_PATH,\n",
    "    symmetrize=False,\n",
    ") -> DataFrame:\n",
    "    \"\"\"Load a saved empirical confusion matrix and format it a bit. Optionally\n",
    "       `symmetrize` off-diagonal values.\"\"\"\n",
    "    cm = pd.read_csv(path)\n",
    "    cm.rename(columns={cm.columns[0]: \"Stimulus\"}, inplace=True)\n",
    "    cm.set_index(\"Stimulus\", drop=True, inplace=True)\n",
    "    if symmetrize: cm = (cm + cm.T) / 2\n",
    "    return cm\n",
    "\n",
    "def get_confusion_matrix(\n",
    "    ocr_fn: str,\n",
    "    n_out = 26,\n",
    "    arch = cornet_for_ocr,\n",
    "    splitter = cornet_splitter,\n",
    "    df = get_sans_serif_df,\n",
    "    tfms = EMPIRICAL_FINETUNING_TFMS,\n",
    "    ds_idx = 0,\n",
    "    iters = 1\n",
    ") -> DataFrame:\n",
    "    \"\"\"Compute a confusion matrix from output propabilites, not mere misclassifications.\"\"\"\n",
    "    ocr_model = load_ocr_model(file=ocr_fn, arch=arch, n_out=n_out)\n",
    "    learn = build_ocr_learner(\n",
    "        model=ocr_model, \n",
    "        splitter=splitter,\n",
    "        df=df,\n",
    "        loss_func=CrossEntropyLossFlat(flatten=False),\n",
    "        **tfms\n",
    "        )\n",
    "    # Force augmentations to also work on the validation dataset, because `RandImgTfms` \n",
    "    # are only applied to the training dataset by default.\n",
    "    for o in learn.dls.valid.after_item: o.split_idx = None\n",
    "    vocab = learn.dls.vocab[0]\n",
    "    preds_by_class = [[] for _ in vocab]\n",
    "    for _ in range(iters):\n",
    "        preds,targets = learn.get_preds(ds_idx=ds_idx)\n",
    "        for p,c in zip(preds,targets): preds_by_class[c.item()].append(p)\n",
    "    for i,p in enumerate(preds_by_class):\n",
    "        preds_by_class[i] = [vocab[i]] + [o.item() for o in torch.stack(p).mean(dim=0)]\n",
    "    cm = DataFrame(preds_by_class)\n",
    "    cm.columns = [\"Stimulus\"] + vocab\n",
    "    cm.set_index(\"Stimulus\", inplace=True, drop=True)\n",
    "    return cm\n",
    "\n",
    "def compare_confusion_matrices(\n",
    "    a: DataFrame,\n",
    "    b: DataFrame,\n",
    "    symmetrize=False,\n",
    "    non_diagonal=False,\n",
    ") -> DataFrame:\n",
    "    \"\"\"Calculate the Pearson correlation between two confusion matrices. For results\n",
    "       comparable with traditional studies, set both `symmetrize` and `non_diagonal`\n",
    "       to `True`.\"\"\"\n",
    "    from scipy.stats import pearsonr\n",
    "    if symmetrize: a,b = (a + a.T) / 2, (b + b.T) / 2\n",
    "    if non_diagonal:\n",
    "        a_nd,b_nd = [],[]\n",
    "        for i in range(len(a)): \n",
    "            a.iloc[i,i],b.iloc[i,i] = 0,0\n",
    "            a_i,b_i = a.iloc[i].values.tolist(),b.iloc[i].values.tolist()\n",
    "            del(a_i[i],b_i[i])\n",
    "            a_nd.append(a_i); b_nd.append(b_i)\n",
    "        index = a.index.tolist()\n",
    "        a,b = DataFrame(a_nd),DataFrame(b_nd)\n",
    "        a.index,b.index = index,index\n",
    "    if symmetrize:\n",
    "        # Remove duplicate entries before calculating total R\n",
    "        lists = [[],[]]\n",
    "        for i,o in enumerate([a,b]):\n",
    "            for j in range(len(o)): \n",
    "                k = j if non_diagonal else j+1\n",
    "                lists[i].append(o.values[j,:k].tolist())\n",
    "        flat_r = pearsonr(*[flatten_list(o) for o in lists])\n",
    "    else: \n",
    "        flat_r = pearsonr(*[flatten_list(o.values.tolist()) for o in (a,b)])\n",
    "    per_cat_r = [(\"Total\",) + flat_r]\n",
    "    for c in a.index:\n",
    "        per_cat_r.append((c,) + pearsonr(*[o.loc[c].values.tolist() for o in (a,b)]))\n",
    "    per_cat_r = pd.DataFrame(per_cat_r)\n",
    "    per_cat_r.columns = [\"Stimulus\", \"Pearson R\", \"p\"]\n",
    "    per_cat_r.set_index(\"Stimulus\", drop=True, inplace=True)\n",
    "    return per_cat_r"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To Do: Oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "\n",
    "# It would be nice to have an oversampling method ready, but the code below is for Fast.ai v1\n",
    "# and would have to be converted to probably something tied to the DataLoader.\n",
    "\n",
    "from torch.utils.data.sampler import WeightedRandomSampler\n",
    "\n",
    "class OverSamplingCallback(Callback):\n",
    "    \"\"\"Oversampling to correct unbalanced dataset.\n",
    "       By Tanishq Mathew Abraham, https://www.kaggle.com/code/tanlikesmath/oversampling-mnist-with-fastai/notebook\"\"\"\n",
    "    def __init__(self,learn:Learner):\n",
    "        super().__init__(learn)\n",
    "        ds = self.learn.data.train_dl.dataset\n",
    "        self.labels = ds.y.items\n",
    "        _, counts = np.unique(self.labels, return_counts=True)\n",
    "        self.weights = torch.DoubleTensor((1/counts)[self.labels])\n",
    "        self.label_counts = np.bincount([self.learn.data.train_dl.dataset.y[i].data \\\n",
    "                                         for i in range(len(ds))])\n",
    "        self.total_len_oversample = int(self.learn.data.c * np.max(self.label_counts))\n",
    "        \n",
    "    def before_train(self, **kwargs):\n",
    "        self.learn.data.train_dl.dl.batch_sampler = BatchSampler(WeightedRandomSampler(self.weights, self.total_len_oversample), \n",
    "                                                                 self.learn.data.train_dl.batch_size,\n",
    "                                                                 False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_ocr_model_training_1.ipynb.\n",
      "Converted 01_fontlearnertests.ipynb.\n",
      "Converted 02_first_letter_and_optim_tests.ipynb.\n",
      "Converted 03_vector_models.ipynb.\n",
      "Converted 04_font_statistics.ipynb.\n",
      "Converted 05_full_aphabet_optimisation.ipynb.\n",
      "Converted 06_data_augmentation.ipynb.\n",
      "Converted 07_optimising_with_augmentation.ipynb.\n",
      "Converted 08_comparing_existing_fonts.ipynb.\n",
      "Converted 09_experiment_1_optimise_to_match_font.ipynb.\n",
      "Converted 10_optimising_with_aug_run_2.ipynb.\n",
      "Converted 11_optimise_to_match_font_multires.ipynb.\n",
      "Converted 12_optimising_with_aug_with_multiple_ocr_models.ipynb.\n",
      "Converted 13_analysing_ocr_models.ipynb.\n",
      "Converted 14_experiment_2_ocrlearner_training_with_emp_data.ipynb.\n",
      "Converted 15_experiment_3_optimise_with_emp_model.ipynb.\n",
      "Converted 16_experiment_2_rerun.ipynb.\n",
      "Converted 17_experiment_3_rerun.ipynb.\n",
      "Converted aifont_core.ipynb.\n",
      "Converted aifont_fontlearner.ipynb.\n",
      "Converted aifont_fontsampler.ipynb.\n",
      "Converted aifont_ocrlearner.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "from nbdev.export import notebook2script; notebook2script()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5710b12fb88680bf60c169aecc91e9487b0350ee3a6536206b6750ffeed12b61"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('ai-font-p3.9')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
