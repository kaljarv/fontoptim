{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# default_exp fontlearner\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Font Learner\n",
    "\n",
    "> Diffvg-based learner for font optimisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from aifont.core import *\n",
    "from enum import Enum\n",
    "from fastai.data.all import *\n",
    "from fastai.vision.all import *\n",
    "import ffmpeg\n",
    "from nbdev.showdoc import *\n",
    "import PIL\n",
    "import pydiffvg\n",
    "from pydiffvg import Polygon, Rect, RenderFunction, ShapeGroup\n",
    "from typing import Callable, List, Protocol, Tuple, Union"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants and Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "COLOR_BLACK  = tensor(0., 0., 0., 1.)\n",
    "COLOR_WHITE  = tensor(1., 1., 1., 1.)\n",
    "EMPTY_TENSOR = tensor([])\n",
    "RGBA_TO_GS   = tensor(0.2989, 0.5870, 0.1140) # Crude NTSC weights for RGB channels\n",
    "tensor(0.)  = tensor(0.)\n",
    "\n",
    "def get_vocab(dls_or_learn: Union[DataLoaders, Learner]) -> List[str]:\n",
    "    \"\"\"Utility for getting the vocab from a Learner or DataLoaders.\"\"\"\n",
    "    if isinstance(dls_or_learn, Learner): dls_or_learn = dls_or_learn.dls\n",
    "    vocab = dls_or_learn.vocab\n",
    "    if type(vocab) == L and type(vocab[0]) == list: \n",
    "        if vocab[0] != vocab[1]: warn(\"The two vocabs in dls do not match! Using the first one.\")\n",
    "        vocab = vocab[0]\n",
    "    return vocab\n",
    "\n",
    "class DebugCB(Callback):\n",
    "    \"\"\"A `Callback` for debugging a `VectorRenderLayer`.\"\"\"\n",
    "    vals = []\n",
    "    grads = []\n",
    "    def before_fit(self):\n",
    "        # self.model[0].params.retain_grad()\n",
    "        pass\n",
    "    def before_step(self):\n",
    "        m = self.model\n",
    "        self.vals.append(m.params.clone())\n",
    "        self.grads.append(m.params.grad.clone())\n",
    "    def plot(self) -> None:\n",
    "        # val = self.model[0].params[0].item()\n",
    "        # grad_df.iloc[(grad_df.Vals - val).abs().argmin()]\n",
    "        num = self.vals[0].size(0)\n",
    "        def _items(tensor_list, idx): return [x[idx].item() for x in tensor_list]\n",
    "        plt.figure(figsize=(10,10))\n",
    "        for i in range(num): plt.scatter(_items(self.vals, i), _items(self.grads, i), label=f\"Param {i}\")\n",
    "        plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rendering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class Scene:\n",
    "    \"\"\"Just a utility to hold the different scene arguments together.\"\"\"\n",
    "    last_seed: int = None\n",
    "    def __init__(self, shapes: list[any], shape_groups: list[ShapeGroup], \n",
    "        canvas_width = 256, canvas_height = 256, samples = 2):\n",
    "        assert shapes is not None and len(shapes) != 0\n",
    "        assert shape_groups is not None and len(shape_groups) != 0\n",
    "        store_attr()\n",
    "\n",
    "    def get_scene_args(self) -> list:\n",
    "        \"\"\"Get the serialize scene for passing to `pydiffvg.RenderFunction`.\"\"\"\n",
    "        return RenderFunction.serialize_scene(self.canvas_width, \n",
    "                                              self.canvas_height, \n",
    "                                              self.shapes, \n",
    "                                              self.shape_groups)\n",
    "\n",
    "    def render(self, seed: int = None, do_render_grad = False) -> Tensor:\n",
    "        \"\"\"Render the scene using pydffiv `RenderFunction` or its\n",
    "           gradient if `do_render_grad`.\"\"\"\n",
    "        if seed is None: seed = random.randint(0, 1e6)\n",
    "        self.last_seed = seed\n",
    "        scene_args = self.get_scene_args()\n",
    "        w = self.canvas_width\n",
    "        h = self.canvas_height\n",
    "        s = self.samples\n",
    "        args = [w, h, s, s, seed, None] + scene_args\n",
    "        rf = RenderFunction\n",
    "        return rf.render_grad(torch.ones(w, h, 4, device=pydiffvg.get_device()), *args) \\\n",
    "               if do_render_grad else rf.apply(*args)\n",
    "\n",
    "    def render_grad(self, seed: int = None) -> Tensor:\n",
    "        \"\"\"Render the gradient as raster.\"\"\"\n",
    "        return self.render(seed=self.last_seed, do_render_grad=True)\n",
    "    \n",
    "add_docs(Scene)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class ImageSaver:\n",
    "    \"\"\"Create a callback to pass to `VectorRenderLayer` as `rendered_callback`\n",
    "       to save rendered images and optionally grads.\"\"\"\n",
    "    canvas_height: int = None\n",
    "    canvas_width: int = None\n",
    "    iter_files: List[str] = []\n",
    "    grad_files: List[str] = []\n",
    "    def __init__(self, folder: str, save_grad = False, iter_name = \"iter\", \n",
    "                 grad_name = \"grad\"):\n",
    "        assert folder is not None\n",
    "        if folder.endswith(\"/\"):\n",
    "            folder = folder[:-1]\n",
    "        store_attr()\n",
    "\n",
    "    def __call__(self, raster: Tensor, batch_i: int, item_i: int, scene: Scene,\n",
    "                 normalize = False, gamma = 2.2) -> None:\n",
    "        suffix = f\"_{batch_i}_{item_i}\"\n",
    "        self.iter_files.append(self.save_image(raster, f\"{self.iter_name}{suffix}\", normalize=normalize, gamma=gamma))\n",
    "        if self.save_grad: \n",
    "            grad = scene.render_grad()\n",
    "            self.grad_files.append(self.save_image(grad, f\"{self.grad_name}{suffix}\"))\n",
    "\n",
    "    def save_image(self, raster: Tensor, filename: str, normalize = False, gamma = 2.2) -> Str:\n",
    "        \"\"\"Save the `raster` tensor as image file and return filename used.\"\"\"\n",
    "        if not self.canvas_height:\n",
    "            self.canvas_height = raster.size(0)\n",
    "            self.canvas_width  = raster.size(1)\n",
    "        fn = f\"{self.folder}/{filename}.png\"\n",
    "        pydiffvg.imwrite(raster.cpu(), fn, normalize=normalize, gamma=gamma)\n",
    "        return fn\n",
    "    \n",
    "    def render_result_video(self, delete_imgs = False, frame_rate=24, grad=False) -> None:\n",
    "        \"\"\"Render intermediate images as a video.\"\"\"\n",
    "        files = self.grad_files if grad else self.iter_files\n",
    "        out = os.path.join(self.folder, \"grads.mp4\" if grad else \"iters.mp4\")\n",
    "        frames = ffmpeg.input('pipe:', r=str(frame_rate))\n",
    "        process = ffmpeg.input(f\"color=c=white:s={self.canvas_width}x{self.canvas_height}\", f=\"lavfi\") \\\n",
    "                        .overlay(frames, eof_action=\"endall\") \\\n",
    "                        .output(out) \\\n",
    "                        .overwrite_output() \\\n",
    "                        .run_async(pipe_stdin=True, quiet=True)\n",
    "        for in_file in files:\n",
    "            with open(in_file, 'rb') as f: process.stdin.write(f.read())\n",
    "        # Close stdin pipe - FFmpeg fininsh encoding the output file.\n",
    "        process.stdin.close()\n",
    "        process.wait()\n",
    "        if delete_imgs: \n",
    "            for f in files: os.remove(f)\n",
    "        print(\"Rendering video done!\")\n",
    "\n",
    "add_docs(ImageSaver)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataLoader\n",
    "\n",
    "Custom DataLoader for getting letter classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class LetterDL(DataLoader):\n",
    "    \"\"\"A dummy data loader for use with font vector optimisation.\n",
    "       Pass the same `vocab` as in the OCR model. Batch size defaults\n",
    "       to `epoch_len`.\"\"\"\n",
    "    current_i = 0\n",
    "    def __init__(self, vocab: CategoryMap, letters: Tuple[str, ...] = (\"A\",), \n",
    "                 epoch_len = 10, bs = 1, **kwargs):\n",
    "        assert vocab is not None\n",
    "        super(LetterDL, self).__init__(bs=bs, n = epoch_len * bs, **kwargs)\n",
    "        self.categorizer = Categorize(vocab=vocab)\n",
    "        store_attr()\n",
    "\n",
    "    def create_item(self, s) -> Tuple[TensorCategory, TensorCategory]:\n",
    "        \"\"\"Return the CategoryTensor for a random letter from `letters`.\"\"\"\n",
    "        if self.current_i == self.n:\n",
    "            self.current_i = 0\n",
    "            stop()\n",
    "        self.current_i += 1\n",
    "        r = self.categorizer.encodes(random.choice(self.letters))\n",
    "        # Return x and y as a copy of x\n",
    "        return r, r.clone()\n",
    "\n",
    "add_docs(LetterDL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector Model\n",
    "\n",
    "The vector model consists of a `FontParamLayer`, which holds the parameters to optimise, and a subclass of `VectorRenderLayer`, which handles the creation of the letter vectors.\n",
    "\n",
    "> Note that the utility of this bisection is tentative, and the params might as well be contained within the `VectorRenderLayer`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Font Parameters\n",
    "\n",
    "Font parameters can be simple or specified to interpolate to certain ranges. \n",
    "\n",
    "The the order they are stored in the parameter layer. For each, a ParamRange tuple containing the min, max and optional mean values is passed, which are used to interpolate the value from the raw value, usually (â€“1, 1), passed by the parameter layer. Most values are treated as fractions, usually of cap height and, for Height, of canvas height. For standard ranges use the PRange enum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "class ParamInterpolator(GetAttr):\n",
    "    \"\"\"Create an interpolator for sigmoid-activated param values passed\n",
    "       by the `FontParamLayer`. Use `asymmetric` for values whose\n",
    "       distribution is highest at `min` and tapers towards `max`.\"\"\"\n",
    "\n",
    "    noop = False\n",
    "\n",
    "    def __init__(self, min=None, max=None, mean=None, asymmetric=False):\n",
    "        if min is None: min = 0.\n",
    "        if max is None: max = 1.\n",
    "        if min == 0 and max == 0 and not asymmetric: self.noop = True\n",
    "        min, max = tensor(min), tensor(max)\n",
    "        self.v_range = max - min\n",
    "        store_attr()\n",
    "\n",
    "    def __call__(self, value: Tensor) -> Tensor:\n",
    "        return self.interpolate(value)\n",
    "\n",
    "    def interpolate(self, value: Tensor) -> Tensor:\n",
    "        \"\"\"Interpolate `value`.\"\"\"\n",
    "        if self.noop: return value\n",
    "        if self.asymmetric: value = 2 * torch.maximum(value - .5, tensor(0.))\n",
    "        return self.min + self.v_range * value\n",
    "\n",
    "# Parameter ranges\n",
    "PRANGE_DEFAULT = ParamInterpolator( 0.0,   1.0)\n",
    "PRANGE_SMALL   = ParamInterpolator( 0.0,   0.25)\n",
    "PRANGE_HALF    = ParamInterpolator( 0.0,   0.5)\n",
    "PRANGE_BIDIR   = ParamInterpolator(-1.0,   1.0)\n",
    "PRANGE_BIDIR_H = ParamInterpolator(-0.5,   0.5)\n",
    "PRANGE_BIDIR_S = ParamInterpolator(-0.25,  0.25)\n",
    "PRANGE_NONZERO = ParamInterpolator( 0.05,  1.0)\n",
    "PRANGE_ASYM    = ParamInterpolator( 0.0,   1.0,  asymmetric=True)\n",
    "PRANGE_ASYM_S  = ParamInterpolator( 0.0,   0.25, asymmetric=True)\n",
    "PRANGE_STROKE  = ParamInterpolator( 0.01,  0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector Rendering Layer Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "class Normaliser(Protocol):\n",
    "    \"\"\"For normalising rasters for the OCR.\"\"\"\n",
    "    mean: float\n",
    "    std: float\n",
    "\n",
    "class VectorRenderLayerBase(Module):\n",
    "    \"\"\"Base for vector render layers. Get's input from a FontParamLayer\n",
    "       and returns the diffvg rendering. Override `create_scenes` in\n",
    "       subclasses and save the results in `self.scenes` of which there\n",
    "       should be `bs`. `forward` calls `render` which renders the\n",
    "       scenes and permutes to match the OCR model. Note that the workflow\n",
    "       is based on greyscale images and we're only using the alpha value\n",
    "       of the diffvg render output.\n",
    "\n",
    "       Params can be defined simply with `n_params` or an `OrderedDict` \n",
    "       `param_specs` that contains each parameter's name and a \n",
    "       `ParamInterpolator` to which the activated param values is passed.\n",
    "       All params have sigmoid activation and, thus, have an output range\n",
    "       of [0, 1] before possible interpolation.\n",
    "       \n",
    "       Init parameters:\n",
    "       `canvas_width`, `canvas_height`: rastered canvas dims\n",
    "       `raster_norm`: use the normaliser from the OCR `dls`\n",
    "       `clip_raster`: whether to clip color values to [0., 1.] as is done when\n",
    "            saving image (note that the values produced by the render\n",
    "            function value wildly up to more than 10. so setting this to\n",
    "            False is advised against)\n",
    "       `apply_gamma`: whether to apply `gamma` to the color values similarly\n",
    "            to clipping above\n",
    "       `n_params`: number of params (with sigmoid activation). Either this or\n",
    "            `param_specs` must be passed.\n",
    "       `param_specs`: `OrderedDict` that contains each parameter's name and a \n",
    "            `ParamInterpolator` to which the activated param values is passed.\n",
    "       `vocab`: list of letter strings to which the inputs are matched\n",
    "       `init_range`: defines the range of the   pre-activation parameter \n",
    "            value space when initialised at random by `reset_parameters` \n",
    "            centered around the middle.\n",
    "       `eps`: amount of random jitter added to params, use with care!\n",
    "       `n_colors_out`: color channels out\n",
    "       `max_distance`: the maximum fraction [0., 1.] of canvas dims \n",
    "            `distance_params` can span\n",
    "       `fixed_seed`: fixed seed value to pass to `pydiffvg.RenderFunction`\n",
    "       `gamma`: set to override default gamma of 2.2 for colour images and 1.\n",
    "            for grayscale ones\n",
    "       `stroke_width`: stroke width for shape generator helpers\n",
    "            (note that this is defined as a fraction of `canvas_size`);\n",
    "            either a float or a tuple of min and max width and used by\n",
    "            `expand_stroke_width`\n",
    "       `stroke_color`: default stroke color for shape generator helpers\n",
    "       `rendered_callback`: set to an ImageSaver to save interim renders\"\"\"\n",
    "    batch_i = -1\n",
    "    bs: int = None\n",
    "    debug = False\n",
    "    i: int # Current item index\n",
    "    params_with_eps: Tensor = None\n",
    "    scenes: List[Scene] = []\n",
    "    sigmoid = torch.nn.Sigmoid()\n",
    "    stored = []\n",
    "    x: Tensor = None\n",
    "    def __init__(self, canvas_width: int, canvas_height: int, vocab: CategoryMap, \n",
    "                 param_specs: OrderedDict = None, n_params: int = None, seed: int = None, \n",
    "                 init_range = 2., eps: float = None, raster_norm: Normaliser = None, clip_raster = True, \n",
    "                 apply_gamma = True, n_colors_out = 1, max_distance = 1., fixed_seed: int = None, \n",
    "                 gamma: float = None, stroke_width: Union[float, Tuple[float, float]] = 1./28, \n",
    "                 stroke_color = COLOR_BLACK, \n",
    "                 rendered_callback: Callable[[Tensor, int, int, Scene, bool, float], None] = None):\n",
    "        if max_distance is None: max_distance = 1.\n",
    "        assert max_distance <= 1.\n",
    "        if param_specs is not None: n_params = len(param_specs)\n",
    "        assert n_params is not None and vocab is not None\n",
    "        if seed is not None: torch.random.manual_seed(seed)\n",
    "        super(VectorRenderLayerBase, self).__init__()\n",
    "        self.param_names = list(param_specs.keys())\n",
    "        self.weight = torch.nn.Parameter(torch.empty(n_params))\n",
    "        self.canvas_size = max(canvas_width, canvas_height)\n",
    "        stroke_width = tensor(stroke_width)\n",
    "        if canvas_width != canvas_height: \n",
    "            warn(f\"When canvas is not square ({canvas_width}x{canvas_height}), \"\n",
    "                  \"some dimensions may be expanded outside it.\")\n",
    "        if gamma is None: gamma = 1. if n_colors_out == 1 else 2.2\n",
    "        store_attr()\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def __repr__(self):\n",
    "        if self.param_specs is None:\n",
    "            p_strings = [f\"  {x.item()} ({self.sigmoid(x).item()})\" for x in self.weight]\n",
    "        else:\n",
    "            p_strings = [f\"  {i} {k}: {self.weight[i].item()} ({self.get_param_by_name(k, no_eps=True).item()})\" \\\n",
    "                         for i,k in enumerate(self.param_specs.keys())]\n",
    "        return \"\\n\".join([f\"{self.__class__.__name__} with params:\"] + p_strings)\n",
    "\n",
    "    @property\n",
    "    def params(self) -> Tensor:\n",
    "        \"\"\"A synomym for `weight`.\"\"\"\n",
    "        return self.weight\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        \"\"\"Randomly init the parameters around the middle of possible values.\"\"\"\n",
    "        self.weight.data.uniform_(-self.init_range, self.init_range)\n",
    "\n",
    "    def get_item(self, i: int) -> Tensor:\n",
    "        \"\"\"Get item `i` in the batch `x`\"\"\"\n",
    "        return self.x[i]\n",
    "\n",
    "    def get_letter(self, i: int = None) -> string:\n",
    "        \"\"\"Get letter as string for item `i`.\"\"\"\n",
    "        if i is None: i = self.i\n",
    "        return self.vocab[self.get_input(i).int().item()]\n",
    "\n",
    "    def get_param_by_name(self, name: str, i: int = None, no_eps = False) -> Tensor:\n",
    "        \"\"\"Get interpolated param value by `name` for item `i`.\"\"\"\n",
    "        assert self.param_specs is not None and name in self.param_specs\n",
    "        idx = self.get_param_index(name)\n",
    "        v = self.get_params(i, no_eps=no_eps)[idx]\n",
    "        return self.param_specs[name](v)\n",
    "\n",
    "    def set_param(self, name: str, value: Union[float, Tensor]) -> None:\n",
    "        \"\"\"Set the value for the named param. Mostly for debugging.\"\"\"\n",
    "        assert self.param_specs is not None and name in self.param_specs\n",
    "        idx = self.get_param_index(name)\n",
    "        self.params.data[idx] = tensor(value)\n",
    "\n",
    "    def get_param_index(self, name: str) -> int:\n",
    "        \"\"\"Get the index of the named param.\"\"\"\n",
    "        return self.param_names.index(name)\n",
    "\n",
    "    def get_params(self, i: int, no_eps = False) -> Tensor:\n",
    "        \"\"\"Get params for item `i` in the batch passed by \n",
    "           `FontParamLayer` as part of `x`\"\"\"\n",
    "        assert not no_eps or i is None, \"When using no_eps, don't use i\"\n",
    "        if i is None and not no_eps: i = self.i\n",
    "        return self.sigmoid(self.params if no_eps or not self.eps else self.params_with_eps[i])\n",
    "        \n",
    "    def get_input(self, i: int) -> Tensor:\n",
    "        \"\"\"Get the letter category for item `i` in `x`\"\"\"\n",
    "        return self.x[i]\n",
    "\n",
    "    def add_eps(self) -> None:\n",
    "        \"\"\"Apply random eps to params, which differs for each item in the batch. \n",
    "           Cf. diffvg/apps/generative modeling/rendering.render_lines\"\"\"\n",
    "        assert self.bs is not None\n",
    "        if not self.eps: return # params_with_eps defaults to params\n",
    "        sz = (self.bs,) + self.params.size()\n",
    "        self.params_with_eps = self.params.expand(sz) + self.eps * torch.randn(sz)\n",
    "\n",
    "    def expand_distance(self, vals: Tensor) -> Tensor:\n",
    "        \"\"\"Expand values to a central `self.max_distance` fraction of the canvas.\n",
    "           Coordinates originate from NW.\"\"\"\n",
    "        return self.canvas_size * vals if self.max_distance == 1. else \\\n",
    "               self.canvas_size * ((1 - self.max_distance) / 2 + vals * self.max_distance)\n",
    "\n",
    "    def expand_stroke_width(self, vals: Tensor = None) -> Tensor:\n",
    "        \"\"\"Expand `vals`,  based on `[min_stroke_width, max_stroke_width] * canvas_height`.\n",
    "           Note that we divide by two so that result is in line with traditional usage in\n",
    "           vector software.\"\"\"\n",
    "        if vals is None: \n",
    "            warn(\"Using default stroke width in rendering.\")\n",
    "            vals = tensor(1.)\n",
    "        w = vals * self.stroke_width if self.stroke_width.ndim == 0 \\\n",
    "            else self.stroke_width[0] + vals * (self.stroke_width[1] - self.stroke_width[0])\n",
    "        return w * self.canvas_size / 2\n",
    "\n",
    "    def normalise_raster(self, raster: Tensor) -> Tensor:\n",
    "        \"\"\"Apply normalisation to `raster`. Not useful for grayscale letters.\"\"\"\n",
    "        if not self.raster_norm: return raster\n",
    "        return (raster - self.raster_norm.mean) / self.raster_norm.std \n",
    "\n",
    "    def forward(self, x) -> Tensor:\n",
    "        \"\"\"Render letters defined in `x`.\"\"\"\n",
    "        # Convert the input x to size (bs, 1) if it's one-dimensional\n",
    "        if x.ndim == 1: x = x.unsqueeze(1)\n",
    "        elif x.ndim != 2: raise ValueError(\"Input can only be 1- or 2-dimensional.\")\n",
    "        self.batch_i += 1\n",
    "        self.x = x\n",
    "        self.bs = x.size(0)\n",
    "        self.add_eps()\n",
    "        self.scenes = [None] * self.bs\n",
    "        self.create_scenes()\n",
    "        return self.render()\n",
    "\n",
    "    def create_scenes(self) -> None:\n",
    "        \"\"\"Override this in subclasses to create the vector scenes for the letters.\"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def create_line_scene(self, *shapes) -> Scene:\n",
    "        \"\"\"Create a simple line-drawing Scene with shapes.\"\"\"\n",
    "        shape_groups = [self.create_line_group(*shapes)]\n",
    "        return self.create_scene_from_groups(shapes, shape_groups)\n",
    "\n",
    "    def create_line_group(self, *shapes, stroke_color=None, id_offset=0) -> ShapeGroup:\n",
    "        \"\"\"Create a ShapeGroup from shapes for line drawing.\"\"\"\n",
    "        assert len(shapes) and type(shapes[0]) not in (tuple, list), \"Unpack shapes\"\n",
    "        if stroke_color is None: stroke_color = self.stroke_color\n",
    "        return ShapeGroup(shape_ids=tensor([x + id_offset for x in range(len(shapes))]),\n",
    "                          fill_color=None,\n",
    "                          stroke_color=stroke_color,\n",
    "                          use_even_odd_rule=False)\n",
    "\n",
    "    def create_fill_group(self, *shapes, fill_color=None, id_offset=0) -> ShapeGroup:\n",
    "        \"\"\"Create a ShapeGroup from shapes for filling.\"\"\"\n",
    "        assert len(shapes) and type(shapes[0]) not in (tuple, list), \"Unpack shapes\"\n",
    "        if fill_color is None: fill_color = self.stroke_color\n",
    "        return ShapeGroup(shape_ids=tensor([x + id_offset for x in range(len(shapes))]),\n",
    "                          fill_color=fill_color,\n",
    "                          stroke_color=None,\n",
    "                          use_even_odd_rule=True)\n",
    "\n",
    "    def create_scene_from_groups(self, shapes, shape_groups) -> list:\n",
    "        \"\"\"Create a scene from `shapes` and `shape_groups`.\"\"\"\n",
    "        # Check that there are no duplicate ids\n",
    "        all_ids = torch.concat([x.shape_ids for x in shape_groups])\n",
    "        assert all_ids.numel() == all_ids.unique().numel()\n",
    "        return Scene(shapes=shapes, shape_groups=shape_groups, \n",
    "                     canvas_width=self.canvas_width, canvas_height=self.canvas_height)\n",
    "\n",
    "    def create_mixed_scene(self, line_shapes=[], fill_shapes=[], stroke_color=None, fill_color=None, bg_color=None) -> list:\n",
    "        \"\"\"Create a scene that has both `line_shapes` and `fill_shapes` and optionally a background.\"\"\"\n",
    "        shapes = line_shapes + fill_shapes\n",
    "        shape_groups = []\n",
    "        id_offset = 0\n",
    "        if bg_color is not None:\n",
    "            bg = Rect(tensor(0., 0.), tensor(self.canvas_width, self.canvas_height), stroke_width=tensor(0.))\n",
    "            shapes.insert(0, bg)\n",
    "            shape_groups.append(self.create_fill_group(bg, fill_color=bg_color, id_offset=id_offset))\n",
    "            id_offset += 1\n",
    "        if len(line_shapes) > 0: \n",
    "            shape_groups.append(self.create_line_group(*line_shapes, stroke_color=stroke_color, id_offset=id_offset))\n",
    "            id_offset += len(line_shapes)\n",
    "        if len(fill_shapes) > 0:\n",
    "            shape_groups.append(self.create_fill_group(*fill_shapes, fill_color=fill_color, id_offset=id_offset))\n",
    "        return self.create_scene_from_groups(shapes, shape_groups)\n",
    "\n",
    "    def create_line_scene_from_points(self, *point_tensors, stroke_width=None, is_closed=False, expand_distance=False) -> Scene:\n",
    "        \"\"\"Shorthand for `create_line_scene` by passing `point_tensors` that\n",
    "           are converted to polygons.\"\"\"\n",
    "        return self.create_line_scene(*self.points_to_polygons(*point_tensors, stroke_width=stroke_width, \n",
    "                                                               is_closed=is_closed, expand_distance=expand_distance))\n",
    "\n",
    "    def points_to_polygons(self, *point_tensors, stroke_width=None, is_closed=False, expand_distance=False) -> List[Polygon]:\n",
    "        \"\"\"Convert `point_tensors` to a List of pydiffvg Polygons.\"\"\"\n",
    "        return [Polygon(points=self.expand_distance(pt) if expand_distance else pt,\n",
    "                        stroke_width=self.expand_stroke_width() if stroke_width is None else stroke_width,\n",
    "                        is_closed=is_closed) \\\n",
    "                for pt in point_tensors]\n",
    "\n",
    "    def render(self) -> Tensor:\n",
    "        \"\"\"Render `self.scenes` as a raster tensor using pydiffvg.\"\"\"\n",
    "        assert self.scenes is not None and len(self.scenes) == self.bs\n",
    "        cols = self.n_colors_out\n",
    "        output = torch.zeros(self.bs, cols, self.canvas_width, self.canvas_height) # .requires_grad_()\n",
    "        for i, s in enumerate(self.scenes):\n",
    "            raster = s.render(seed=self.fixed_seed)\n",
    "            if self.rendered_callback: \n",
    "                self.rendered_callback(raster=raster, batch_i=self.batch_i, item_i=i, scene=s, normalize=False, gamma=self.gamma)\n",
    "            if cols in (1, 3):\n",
    "                # Output is w,h,rgba, where with values in 0.-1. (and black thus 0., 0., 0., 1.)\n",
    "                # First, we apply alpha by mixing output with white in that proportion\n",
    "                if self.debug: self.stored.append(raster.clone())\n",
    "                alpha = raster[:, :, 3].unsqueeze(2).expand(-1, -1, 4)\n",
    "                white = torch.full_like(raster, 1.) * (1. - alpha)\n",
    "                raster = (white + raster * alpha)[:, :, :3] # Now raster is w,h,rgb\n",
    "                if cols == 1: # Convert to grayscale if needed\n",
    "                    raster *= RGBA_TO_GS # This is a crude NTSC sampling to grayscale\n",
    "                    raster = raster.sum(-1, keepdims=True)\n",
    "                raster = raster.permute(2, 0, 1) # Order channel-first\n",
    "            elif cols != 4: raise NotImplementedError(f\"n_colors_out '{cols}' can only be 1, 3 or 4.\")\n",
    "            raster = self.normalise_raster(raster)\n",
    "            if self.clip_raster: raster = raster.clip(0., 1.)\n",
    "            if self.apply_gamma: \n",
    "                if cols == 1: raster = raster.pow(1.0/self.gamma)\n",
    "                else: raster[:,:,:3] = raster[:,:,:3].pow(1.0/self.gamma)\n",
    "            # assert raster.requires_grad\n",
    "            output[i] = raster\n",
    "        return output\n",
    "\n",
    "add_docs(VectorRenderLayerBase)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class OCRLoss(CrossEntropyLossFlat):\n",
    "    \"\"\"Softmaxed CrossEntropyLossFlat between `ocr_model`'s prediction\n",
    "       and target category. Use after `VectorRenderLayerBase`.\"\"\"\n",
    "    stored: List[Tuple[float, int, float, Tensor]] = []\n",
    "    def __init__(self, ocr_model, debug = False, **kwargs):\n",
    "        super(OCRLoss, self).__init__(**kwargs)\n",
    "        assert ocr_model is not None\n",
    "        ocr_model.eval()\n",
    "        store_attr(\"ocr_model, debug\")\n",
    "\n",
    "    def __call__(self, inp, target):\n",
    "        pred = self.activation(self.ocr_model(inp))\n",
    "        loss = super(OCRLoss, self).__call__(pred, target)\n",
    "        if self.debug: self.stored.append((loss.item(), pred[0].argmax().item(), pred[0].max().item(), pred[0].detach()))\n",
    "        return loss\n",
    "\n",
    "def param_loss(x: Tensor, loss_start=4., loss_factor=1.) -> Tensor:\n",
    "    \"\"\"Calculate a linear loss for abs values above `loss_start` multiplied\n",
    "       by `loss_factor`.\"\"\"\n",
    "    return loss_factor * torch.maximum(x.abs() - loss_start, tensor(0.)).sum()\n",
    "\n",
    "class ParamLoss(Module):\n",
    "    \"\"\"Calculate a loss based on extreme parameter values.\"\"\"\n",
    "    def __init__(self, vector_model: VectorRenderLayerBase, loss_start=4., loss_factor=1., **kwargs):\n",
    "        super(ParamLoss, self).__init__(**kwargs)\n",
    "        assert vector_model is not None\n",
    "        store_attr(\"vector_model,loss_start,loss_factor\")\n",
    "\n",
    "    def forward(self, *args):\n",
    "        return param_loss(self.vector_model.params, self.loss_start, self.loss_factor)\n",
    "        \n",
    "class OCRAndParamLoss(Module):\n",
    "    \"\"\"Combined OCR and param loss.\"\"\"\n",
    "    def __init__(self, ocr_model, vector_model: VectorRenderLayerBase, loss_start=4., loss_factor=1., debug = False, **kwargs):\n",
    "        super(OCRAndParamLoss, self).__init__(**kwargs)\n",
    "        self.ocr_loss = OCRLoss(ocr_model=ocr_model, debug=debug, **kwargs)\n",
    "        self.param_loss = ParamLoss(vector_model=vector_model, loss_start=loss_start, loss_factor=loss_factor, **kwargs)\n",
    "\n",
    "    def forward(self, inp, target):\n",
    "        return self.ocr_loss(inp, target) + self.param_loss(inp, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class VectorLearner(Learner):\n",
    "    \"\"\"A simple extension to Learner offering some utility methods.\"\"\"\n",
    "    def __init__(self, image_saver=None, **kwargs):\n",
    "        super(VectorLearner, self).__init__(**kwargs)\n",
    "        store_attr(\"image_saver\")\n",
    "    \n",
    "    @property\n",
    "    def vocab(self) -> List[str]:\n",
    "        return self.dls.vocab\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        \"\"\"Shortcut for `self.model.reset_parameters`.\"\"\"\n",
    "        self.model.reset_parameters()\n",
    "\n",
    "    def set_param(self, *args, **kwargs):\n",
    "        \"\"\"Shortcut for `self.model.set_param`.\"\"\"\n",
    "        self.model.set_param(*args, **kwargs)\n",
    "\n",
    "    def render_letter(self, letter: str = \"A\", scale: float = None) -> PIL.Image:\n",
    "        \"\"\"Render a letter using the current vector model.\"\"\"\n",
    "        inp = tensor([self.vocab.index(letter)])\n",
    "        m = self.model\n",
    "        trn = m.training\n",
    "        m.eval()\n",
    "        with torch.no_grad(): img = m(inp).squeeze().clip(0., 1.) * 255\n",
    "        m.train(trn)\n",
    "        pil_img = PILImageBW.create(img).convert('RGB')\n",
    "        if scale is not None: pil_img = pil_img.resize((round(pil_img.width * scale), round(pil_img.height * scale)), resample=0)\n",
    "        return pil_img\n",
    "\n",
    "    def render_result_video(self, **kwargs):\n",
    "        \"\"\"Shortcut for `self.image_saver.render_result_video`\"\"\"\n",
    "        assert self.image_saver is not None\n",
    "        self.image_saver.render_result_video(**kwargs)\n",
    "\n",
    "    def calculate_losses(self, n = 20, param_ranges: List[Union[Tuple[float, float], float]] = None) -> pd.DataFrame:\n",
    "        \"\"\"Output loss statistics and predictions for different param values.\n",
    "           If `param_ranges` is supplied, it should contain the min and max\n",
    "           values to use for each parameter or a fixed value.\"\"\"\n",
    "        assert n > 1\n",
    "        assert self.loss_func.debug, \"Debug must be enabled for the loss function.\"\n",
    "        model = self.model\n",
    "        model.eval()\n",
    "        pl = model[0]\n",
    "        n_pars = pl.n_params\n",
    "        param_f = torch.full((n_pars,), 4)  if param_ranges is None else tensor([0. if type(x) is float else x[1] - x[0] for x in param_ranges])\n",
    "        param_c = torch.full((n_pars,), -2) if param_ranges is None else tensor([x  if type(x) is float else x[0] for x in param_ranges])\n",
    "        x,y = self.dls.one_batch()\n",
    "        stats = []\n",
    "        for i in range(n):\n",
    "            p_vals = param_c + param_f * i / (n - 1)\n",
    "            pl.params.data = p_vals\n",
    "            # Vector prediction\n",
    "            p = model(x)\n",
    "            _ = self.loss_func(p, y)\n",
    "            d = {\n",
    "                \"loss\": self.loss_func.stored[-1][0],\n",
    "                \"pred\": self.vocab[self.loss_func.stored[-1][1]],\n",
    "                \"pred_activation\": self.loss_func.stored[-1][2]\n",
    "                }\n",
    "            for j in range(n_pars): d[f\"param_{j}\"] = p_vals[j].item()\n",
    "            stats.append(d)\n",
    "        return pd.DataFrame(stats)\n",
    "\n",
    "add_docs(VectorLearner)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 01_fontlearnertests.ipynb.\n",
      "Converted 02_lettervectors.ipynb.\n",
      "Converted aifont_core.ipynb.\n",
      "Converted aifont_fontlearner.ipynb.\n",
      "Converted aifont_fontsampler.ipynb.\n",
      "Converted aifont_ocrlearner.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "from nbdev.export import notebook2script; notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5710b12fb88680bf60c169aecc91e9487b0350ee3a6536206b6750ffeed12b61"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('ai-font-p3.9')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
