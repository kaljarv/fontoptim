{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# default_exp fontlearner\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Font Learner\n",
    "\n",
    "> Diffvg-based learner for font optimisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from aifont.core import *\n",
    "from enum import Enum\n",
    "from fastai.data.all import *\n",
    "from fastai.vision.all import *\n",
    "import ffmpeg\n",
    "from nbdev.showdoc import *\n",
    "import PIL\n",
    "import pydiffvg\n",
    "from pydiffvg import Polygon, Rect, RenderFunction, ShapeGroup\n",
    "from typing import Callable, List, Protocol, Tuple, Union"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants and Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "COLOR_BLACK  = tensor(0., 0., 0., 1.)\n",
    "COLOR_WHITE  = tensor(1., 1., 1., 1.)\n",
    "EMPTY_TENSOR = tensor([])\n",
    "RGBA_TO_GS   = tensor(0.2989, 0.5870, 0.1140) # Crude NTSC weights for RGB channels\n",
    "tensor(0.)  = tensor(0.)\n",
    "\n",
    "def get_vocab(dls_or_learn: Union[DataLoaders, Learner]) -> List[str]:\n",
    "    \"\"\"Utility for getting the vocab from a Learner or DataLoaders.\"\"\"\n",
    "    if isinstance(dls_or_learn, Learner): dls_or_learn = dls_or_learn.dls\n",
    "    vocab = dls_or_learn.vocab\n",
    "    if type(vocab) == L and type(vocab[0]) == list: \n",
    "        if vocab[0] != vocab[1]: warn(\"The two vocabs in dls do not match! Using the first one.\")\n",
    "        vocab = vocab[0]\n",
    "    return vocab\n",
    "\n",
    "class DebugCB(Callback):\n",
    "    \"\"\"A `Callback` for debugging a `VectorRenderLayer`.\"\"\"\n",
    "    vals = []\n",
    "    grads = []\n",
    "    def before_fit(self):\n",
    "        # self.model[0].params.retain_grad()\n",
    "        pass\n",
    "    def before_step(self):\n",
    "        m = self.model\n",
    "        self.vals.append(m.params.clone())\n",
    "        self.grads.append(m.params.grad.clone())\n",
    "    def plot(self) -> None:\n",
    "        # val = self.model[0].params[0].item()\n",
    "        # grad_df.iloc[(grad_df.Vals - val).abs().argmin()]\n",
    "        num = self.vals[0].size(0)\n",
    "        def _items(tensor_list, idx): return [x[idx].item() for x in tensor_list]\n",
    "        plt.figure(figsize=(10,10))\n",
    "        for i in range(num): plt.scatter(_items(self.vals, i), _items(self.grads, i), label=f\"Param {i}\")\n",
    "        plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rendering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class Scene:\n",
    "    \"\"\"Just a utility to hold the different scene arguments together.\"\"\"\n",
    "    last_seed: int = None\n",
    "    def __init__(self, shapes: list[any], shape_groups: list[ShapeGroup], \n",
    "        canvas_width = 256, canvas_height = 256, samples = 2):\n",
    "        assert shapes is not None and len(shapes) != 0\n",
    "        assert shape_groups is not None and len(shape_groups) != 0\n",
    "        store_attr()\n",
    "\n",
    "    def get_scene_args(self) -> list:\n",
    "        \"\"\"Get the serialize scene for passing to `pydiffvg.RenderFunction`.\"\"\"\n",
    "        return RenderFunction.serialize_scene(self.canvas_width, \n",
    "                                              self.canvas_height, \n",
    "                                              self.shapes, \n",
    "                                              self.shape_groups)\n",
    "\n",
    "    def render(self, seed: int = None, do_render_grad = False) -> Tensor:\n",
    "        \"\"\"Render the scene using pydffiv `RenderFunction` or its\n",
    "           gradient if `do_render_grad`.\"\"\"\n",
    "        if seed is None: seed = random.randint(0, 1e6)\n",
    "        self.last_seed = seed\n",
    "        scene_args = self.get_scene_args()\n",
    "        w = self.canvas_width\n",
    "        h = self.canvas_height\n",
    "        s = self.samples\n",
    "        args = [w, h, s, s, seed, None] + scene_args\n",
    "        rf = RenderFunction\n",
    "        return rf.render_grad(torch.ones(w, h, 4, device=pydiffvg.get_device()), *args) \\\n",
    "               if do_render_grad else rf.apply(*args)\n",
    "\n",
    "    def render_grad(self, seed: int = None) -> Tensor:\n",
    "        \"\"\"Render the gradient as raster.\"\"\"\n",
    "        return self.render(seed=self.last_seed, do_render_grad=True)\n",
    "    \n",
    "add_docs(Scene)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class ImageSaver:\n",
    "    \"\"\"Create a callback to pass to `VectorRenderLayer` as `rendered_callback`\n",
    "       to save rendered images and optionally grads.\"\"\"\n",
    "    canvas_height: int = None\n",
    "    canvas_width: int = None\n",
    "    iter_files: List[str] = []\n",
    "    grad_files: List[str] = []\n",
    "    def __init__(self, folder: str, save_grad = False, iter_name = \"iter\", \n",
    "                 grad_name = \"grad\"):\n",
    "        assert folder is not None\n",
    "        if folder.endswith(\"/\"):\n",
    "            folder = folder[:-1]\n",
    "        store_attr()\n",
    "\n",
    "    def __call__(self, raster: Tensor, batch_i: int, item_i: int, scene: Scene,\n",
    "                 normalize = False, gamma = 2.2) -> None:\n",
    "        suffix = f\"_{batch_i}_{item_i}\"\n",
    "        self.iter_files.append(self.save_image(raster, f\"{self.iter_name}{suffix}\", normalize=normalize, gamma=gamma))\n",
    "        if self.save_grad: \n",
    "            grad = scene.render_grad()\n",
    "            self.grad_files.append(self.save_image(grad, f\"{self.grad_name}{suffix}\"))\n",
    "\n",
    "    def save_image(self, raster: Tensor, filename: str, normalize = False, gamma = 2.2) -> Str:\n",
    "        \"\"\"Save the `raster` tensor as image file and return filename used.\"\"\"\n",
    "        if not self.canvas_height:\n",
    "            self.canvas_height = raster.size(0)\n",
    "            self.canvas_width  = raster.size(1)\n",
    "        fn = f\"{self.folder}/{filename}.png\"\n",
    "        pydiffvg.imwrite(raster.cpu(), fn, normalize=normalize, gamma=gamma)\n",
    "        return fn\n",
    "    \n",
    "    def render_result_video(self, delete_imgs = False, frame_rate=24, grad=False) -> None:\n",
    "        \"\"\"Render intermediate images as a video.\"\"\"\n",
    "        files = self.grad_files if grad else self.iter_files\n",
    "        out = os.path.join(self.folder, \"grads.mp4\" if grad else \"iters.mp4\")\n",
    "        frames = ffmpeg.input('pipe:', r=str(frame_rate))\n",
    "        process = ffmpeg.input(f\"color=c=white:s={self.canvas_width}x{self.canvas_height}\", f=\"lavfi\") \\\n",
    "                        .overlay(frames, eof_action=\"endall\") \\\n",
    "                        .output(out) \\\n",
    "                        .overwrite_output() \\\n",
    "                        .run_async(pipe_stdin=True, quiet=True)\n",
    "        for in_file in files:\n",
    "            with open(in_file, 'rb') as f: process.stdin.write(f.read())\n",
    "        # Close stdin pipe - FFmpeg fininsh encoding the output file.\n",
    "        process.stdin.close()\n",
    "        process.wait()\n",
    "        if delete_imgs: \n",
    "            for f in files: os.remove(f)\n",
    "        print(\"Rendering video done!\")\n",
    "\n",
    "add_docs(ImageSaver)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataLoader\n",
    "\n",
    "Custom DataLoader for getting letter classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class LetterDL(DataLoader):\n",
    "    \"\"\"A dummy data loader for use with font vector optimisation.\n",
    "       Pass the same `vocab` as in the OCR model. Batch size defaults\n",
    "       to `epoch_len`.\"\"\"\n",
    "    current_i = 0\n",
    "    def __init__(self, vocab: CategoryMap, letters: Tuple[str, ...] = (\"A\",), \n",
    "                 epoch_len = 10, bs = 1, **kwargs):\n",
    "        assert vocab is not None\n",
    "        super(LetterDL, self).__init__(bs=bs, n = epoch_len * bs, **kwargs)\n",
    "        self.categorizer = Categorize(vocab=vocab)\n",
    "        store_attr()\n",
    "\n",
    "    def create_item(self, s) -> Tuple[TensorCategory, TensorCategory]:\n",
    "        \"\"\"Return the CategoryTensor for a random letter from `letters`.\"\"\"\n",
    "        if self.current_i == self.n:\n",
    "            self.current_i = 0\n",
    "            stop()\n",
    "        self.current_i += 1\n",
    "        r = self.categorizer.encodes(random.choice(self.letters))\n",
    "        # Return x and y as a copy of x\n",
    "        return r, r.clone()\n",
    "\n",
    "add_docs(LetterDL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector Model\n",
    "\n",
    "The vector model consists of a `FontParamLayer`, which holds the parameters to optimise, and a subclass of `VectorRenderLayer`, which handles the creation of the letter vectors.\n",
    "\n",
    "> Note that the utility of this bisection is tentative, and the params might as well be contained within the `VectorRenderLayer`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Font Parameters\n",
    "\n",
    "Font parameters can be simple or specified to interpolate to certain ranges. \n",
    "\n",
    "The the order they are stored in the parameter layer. For each, a ParamRange tuple containing the min, max and optional mean values is passed, which are used to interpolate the value from the raw value, usually (–1, 1), passed by the parameter layer. Most values are treated as fractions, usually of cap height and, for Height, of canvas height. For standard ranges use the PRange enum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "class ParamInterpolator(GetAttr):\n",
    "    \"\"\"Create an interpolator for sigmoid-activated param values passed\n",
    "       by the `FontParamLayer`. Use `asymmetric` for values whose\n",
    "       distribution is highest at `min` and tapers towards `max`.\"\"\"\n",
    "\n",
    "    noop = False\n",
    "\n",
    "    def __init__(self, min=None, max=None, mean=None, asymmetric=False):\n",
    "        if min is None: min = 0.\n",
    "        if max is None: max = 1.\n",
    "        if min == 0 and max == 0 and not asymmetric: self.noop = True\n",
    "        min, max = tensor(min), tensor(max)\n",
    "        self.v_range = max - min\n",
    "        store_attr()\n",
    "\n",
    "    def __call__(self, value: Tensor) -> Tensor:\n",
    "        return self.interpolate(value)\n",
    "\n",
    "    def interpolate(self, value: Tensor) -> Tensor:\n",
    "        \"\"\"Interpolate `value`.\"\"\"\n",
    "        if self.noop: return value\n",
    "        if self.asymmetric: value = 2 * torch.maximum(value - .5, tensor(0.))\n",
    "        return self.min + self.v_range * value\n",
    "\n",
    "# Parameter ranges\n",
    "PRANGE_DEFAULT = ParamInterpolator( 0.0,   1.0)\n",
    "PRANGE_SMALL   = ParamInterpolator( 0.0,   0.25)\n",
    "PRANGE_HALF    = ParamInterpolator( 0.0,   0.5)\n",
    "PRANGE_BIDIR   = ParamInterpolator(-1.0,   1.0)\n",
    "PRANGE_BIDIR_H = ParamInterpolator(-0.5,   0.5)\n",
    "PRANGE_BIDIR_S = ParamInterpolator(-0.25,  0.25)\n",
    "PRANGE_NONZERO = ParamInterpolator( 0.05,  1.0)\n",
    "PRANGE_ASYM    = ParamInterpolator( 0.0,   1.0,  asymmetric=True)\n",
    "PRANGE_ASYM_S  = ParamInterpolator( 0.0,   0.25, asymmetric=True)\n",
    "PRANGE_STROKE  = ParamInterpolator( 0.01,  0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector Rendering Layer Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "class Normaliser(Protocol):\n",
    "    \"\"\"For normalising rasters for the OCR.\"\"\"\n",
    "    mean: float\n",
    "    std: float\n",
    "\n",
    "class VectorRenderLayerBase(Module):\n",
    "    \"\"\"Base for vector render layers. Get's input from a FontParamLayer\n",
    "       and returns the diffvg rendering. Override `create_scenes` in\n",
    "       subclasses and save the results in `self.scenes` of which there\n",
    "       should be `bs`. `forward` calls `render` which renders the\n",
    "       scenes and permutes to match the OCR model. Note that the workflow\n",
    "       is based on greyscale images and we're only using the alpha value\n",
    "       of the diffvg render output.\n",
    "\n",
    "       Params can be defined simply with `n_params` or an `OrderedDict` \n",
    "       `param_specs` that contains each parameter's name and a \n",
    "       `ParamInterpolator` to which the activated param values is passed.\n",
    "       All params have sigmoid activation and, thus, have an output range\n",
    "       of [0, 1] before possible interpolation.\n",
    "       \n",
    "       Init parameters:\n",
    "       `canvas_width`, `canvas_height`: rastered canvas dims\n",
    "       `raster_norm`: use the normaliser from the OCR `dls`\n",
    "       `clip_raster`: whether to clip color values to [0., 1.] as is done when\n",
    "            saving image (note that the values produced by the render\n",
    "            function value wildly up to more than 10. so setting this to\n",
    "            False is advised against)\n",
    "       `apply_gamma`: whether to apply `gamma` to the color values similarly\n",
    "            to clipping above\n",
    "       `n_params`: number of params (with sigmoid activation). Either this or\n",
    "            `param_specs` must be passed.\n",
    "       `param_specs`: `OrderedDict` that contains each parameter's name and a \n",
    "            `ParamInterpolator` to which the activated param values is passed.\n",
    "       `vocab`: list of letter strings to which the inputs are matched\n",
    "       `init_range`: defines the range of the   pre-activation parameter \n",
    "            value space when initialised at random by `reset_parameters` \n",
    "            centered around the middle.\n",
    "       `eps`: amount of random jitter added to params, use with care!\n",
    "       `n_colors_out`: color channels out\n",
    "       `max_distance`: the maximum fraction [0., 1.] of canvas dims \n",
    "            `distance_params` can span\n",
    "       `fixed_seed`: fixed seed value to pass to `pydiffvg.RenderFunction`\n",
    "       `gamma`: set to override default gamma of 2.2 for colour images and 1.\n",
    "            for grayscale ones\n",
    "       `stroke_width`: stroke width for shape generator helpers\n",
    "            (note that this is defined as a fraction of `canvas_size`);\n",
    "            either a float or a tuple of min and max width and used by\n",
    "            `expand_stroke_width`\n",
    "       `stroke_color`: default stroke color for shape generator helpers\n",
    "       `rendered_callback`: set to an ImageSaver to save interim renders\"\"\"\n",
    "    batch_i = -1\n",
    "    bs: int = None\n",
    "    debug = False\n",
    "    i: int # Current item index\n",
    "    params_with_eps: Tensor = None\n",
    "    scenes: List[Scene] = []\n",
    "    sigmoid = torch.nn.Sigmoid()\n",
    "    stored = []\n",
    "    x: Tensor = None\n",
    "    def __init__(self, canvas_width: int, canvas_height: int, vocab: CategoryMap, \n",
    "                 param_specs: OrderedDict = None, n_params: int = None, seed: int = None, \n",
    "                 init_range = 2., eps: float = None, raster_norm: Normaliser = None, clip_raster = True, \n",
    "                 apply_gamma = True, n_colors_out = 1, max_distance = 1., fixed_seed: int = None, \n",
    "                 gamma: float = None, stroke_width: Union[float, Tuple[float, float]] = 1./28, \n",
    "                 stroke_color = COLOR_BLACK, \n",
    "                 rendered_callback: Callable[[Tensor, int, int, Scene, bool, float], None] = None):\n",
    "        if max_distance is None: max_distance = 1.\n",
    "        assert max_distance <= 1.\n",
    "        if param_specs is not None: n_params = len(param_specs)\n",
    "        assert n_params is not None and vocab is not None\n",
    "        if seed is not None: torch.random.manual_seed(seed)\n",
    "        super(VectorRenderLayerBase, self).__init__()\n",
    "        self.param_names = list(param_specs.keys())\n",
    "        self.weight = torch.nn.Parameter(torch.empty(n_params))\n",
    "        self.canvas_size = max(canvas_width, canvas_height)\n",
    "        stroke_width = tensor(stroke_width)\n",
    "        if canvas_width != canvas_height: \n",
    "            warn(f\"When canvas is not square ({canvas_width}x{canvas_height}), \"\n",
    "                  \"some dimensions may be expanded outside it.\")\n",
    "        if gamma is None: gamma = 1. if n_colors_out == 1 else 2.2\n",
    "        store_attr()\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def __repr__(self):\n",
    "        if self.param_specs is None:\n",
    "            p_strings = [f\"  {x.item()} ({self.sigmoid(x).item()})\" for x in self.weight]\n",
    "        else:\n",
    "            p_strings = [f\"  {i} {k}: {self.weight[i].item()} ({self.get_param_by_name(k, no_eps=True).item()})\" \\\n",
    "                         for i,k in enumerate(self.param_specs.keys())]\n",
    "        return \"\\n\".join([f\"{self.__class__.__name__} with params:\"] + p_strings)\n",
    "\n",
    "    @property\n",
    "    def params(self) -> Tensor:\n",
    "        \"\"\"A synomym for `weight`.\"\"\"\n",
    "        return self.weight\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        \"\"\"Randomly init the parameters around the middle of possible values.\"\"\"\n",
    "        self.weight.data.uniform_(-self.init_range, self.init_range)\n",
    "\n",
    "    def get_item(self, i: int) -> Tensor:\n",
    "        \"\"\"Get item `i` in the batch `x`\"\"\"\n",
    "        return self.x[i]\n",
    "\n",
    "    def get_letter(self, i: int = None) -> string:\n",
    "        \"\"\"Get letter as string for item `i`.\"\"\"\n",
    "        if i is None: i = self.i\n",
    "        return self.vocab[self.get_input(i).int().item()]\n",
    "\n",
    "    def get_param_by_name(self, name: str, i: int = None, no_eps = False) -> Tensor:\n",
    "        \"\"\"Get interpolated param value by `name` for item `i`.\"\"\"\n",
    "        assert self.param_specs is not None and name in self.param_specs\n",
    "        idx = self.get_param_index(name)\n",
    "        v = self.get_params(i, no_eps=no_eps)[idx]\n",
    "        return self.param_specs[name](v)\n",
    "\n",
    "    def set_param(self, name: str, value: Union[float, Tensor]) -> None:\n",
    "        \"\"\"Set the value for the named param. Mostly for debugging.\"\"\"\n",
    "        assert self.param_specs is not None and name in self.param_specs\n",
    "        idx = self.get_param_index(name)\n",
    "        self.params.data[idx] = tensor(value)\n",
    "\n",
    "    def get_param_index(self, name: str) -> int:\n",
    "        \"\"\"Get the index of the named param.\"\"\"\n",
    "        return self.param_names.index(name)\n",
    "\n",
    "    def get_params(self, i: int, no_eps = False) -> Tensor:\n",
    "        \"\"\"Get params for item `i` in the batch passed by \n",
    "           `FontParamLayer` as part of `x`\"\"\"\n",
    "        assert not no_eps or i is None, \"When using no_eps, don't use i\"\n",
    "        if i is None and not no_eps: i = self.i\n",
    "        return self.sigmoid(self.params if no_eps or not self.eps else self.params_with_eps[i])\n",
    "        \n",
    "    def get_input(self, i: int) -> Tensor:\n",
    "        \"\"\"Get the letter category for item `i` in `x`\"\"\"\n",
    "        return self.x[i]\n",
    "\n",
    "    def add_eps(self) -> None:\n",
    "        \"\"\"Apply random eps to params, which differs for each item in the batch. \n",
    "           Cf. diffvg/apps/generative modeling/rendering.render_lines\"\"\"\n",
    "        assert self.bs is not None\n",
    "        if not self.eps: return # params_with_eps defaults to params\n",
    "        sz = (self.bs,) + self.params.size()\n",
    "        self.params_with_eps = self.params.expand(sz) + self.eps * torch.randn(sz)\n",
    "\n",
    "    def expand_distance(self, vals: Tensor) -> Tensor:\n",
    "        \"\"\"Expand values to a central `self.max_distance` fraction of the canvas.\n",
    "           Coordinates originate from NW.\"\"\"\n",
    "        return self.canvas_size * vals if self.max_distance == 1. else \\\n",
    "               self.canvas_size * ((1 - self.max_distance) / 2 + vals * self.max_distance)\n",
    "\n",
    "    def expand_stroke_width(self, vals: Tensor = None) -> Tensor:\n",
    "        \"\"\"Expand `vals`,  based on `[min_stroke_width, max_stroke_width] * canvas_height`.\n",
    "           Note that we divide by two so that result is in line with traditional usage in\n",
    "           vector software.\"\"\"\n",
    "        if vals is None: \n",
    "            warn(\"Using default stroke width in rendering.\")\n",
    "            vals = tensor(1.)\n",
    "        w = vals * self.stroke_width if self.stroke_width.ndim == 0 \\\n",
    "            else self.stroke_width[0] + vals * (self.stroke_width[1] - self.stroke_width[0])\n",
    "        return w * self.canvas_size / 2\n",
    "\n",
    "    def normalise_raster(self, raster: Tensor) -> Tensor:\n",
    "        \"\"\"Apply normalisation to `raster`. Not useful for grayscale letters.\"\"\"\n",
    "        if not self.raster_norm: return raster\n",
    "        return (raster - self.raster_norm.mean) / self.raster_norm.std \n",
    "\n",
    "    def forward(self, x) -> Tensor:\n",
    "        \"\"\"Render letters defined in `x`.\"\"\"\n",
    "        # Convert the input x to size (bs, 1) if it's one-dimensional\n",
    "        if x.ndim == 1: x = x.unsqueeze(1)\n",
    "        elif x.ndim != 2: raise ValueError(\"Input can only be 1- or 2-dimensional.\")\n",
    "        self.batch_i += 1\n",
    "        self.x = x\n",
    "        self.bs = x.size(0)\n",
    "        self.add_eps()\n",
    "        self.scenes = [None] * self.bs\n",
    "        self.create_scenes()\n",
    "        return self.render()\n",
    "\n",
    "    def create_scenes(self) -> None:\n",
    "        \"\"\"Override this in subclasses to create the vector scenes for the letters.\"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def create_line_scene(self, *shapes) -> Scene:\n",
    "        \"\"\"Create a simple line-drawing Scene with shapes.\"\"\"\n",
    "        shape_groups = [self.create_line_group(*shapes)]\n",
    "        return self.create_scene_from_groups(shapes, shape_groups)\n",
    "\n",
    "    def create_line_group(self, *shapes, stroke_color=None, id_offset=0) -> ShapeGroup:\n",
    "        \"\"\"Create a ShapeGroup from shapes for line drawing.\"\"\"\n",
    "        assert len(shapes) and type(shapes[0]) not in (tuple, list), \"Unpack shapes\"\n",
    "        if stroke_color is None: stroke_color = self.stroke_color\n",
    "        return ShapeGroup(shape_ids=tensor([x + id_offset for x in range(len(shapes))]),\n",
    "                          fill_color=None,\n",
    "                          stroke_color=stroke_color,\n",
    "                          use_even_odd_rule=False)\n",
    "\n",
    "    def create_fill_group(self, *shapes, fill_color=None, id_offset=0) -> ShapeGroup:\n",
    "        \"\"\"Create a ShapeGroup from shapes for filling.\"\"\"\n",
    "        assert len(shapes) and type(shapes[0]) not in (tuple, list), \"Unpack shapes\"\n",
    "        if fill_color is None: fill_color = self.stroke_color\n",
    "        return ShapeGroup(shape_ids=tensor([x + id_offset for x in range(len(shapes))]),\n",
    "                          fill_color=fill_color,\n",
    "                          stroke_color=None,\n",
    "                          use_even_odd_rule=True)\n",
    "\n",
    "    def create_scene_from_groups(self, shapes, shape_groups) -> list:\n",
    "        \"\"\"Create a scene from `shapes` and `shape_groups`.\"\"\"\n",
    "        # Check that there are no duplicate ids\n",
    "        all_ids = torch.concat([x.shape_ids for x in shape_groups])\n",
    "        assert all_ids.numel() == all_ids.unique().numel()\n",
    "        return Scene(shapes=shapes, shape_groups=shape_groups, \n",
    "                     canvas_width=self.canvas_width, canvas_height=self.canvas_height)\n",
    "\n",
    "    def create_mixed_scene(self, line_shapes=[], fill_shapes=[], stroke_color=None, fill_color=None, bg_color=None) -> list:\n",
    "        \"\"\"Create a scene that has both `line_shapes` and `fill_shapes` and optionally a background.\"\"\"\n",
    "        shapes = line_shapes + fill_shapes\n",
    "        shape_groups = []\n",
    "        id_offset = 0\n",
    "        if bg_color is not None:\n",
    "            bg = Rect(tensor(0., 0.), tensor(self.canvas_width, self.canvas_height), stroke_width=tensor(0.))\n",
    "            shapes.insert(0, bg)\n",
    "            shape_groups.append(self.create_fill_group(bg, fill_color=bg_color, id_offset=id_offset))\n",
    "            id_offset += 1\n",
    "        if len(line_shapes) > 0: \n",
    "            shape_groups.append(self.create_line_group(*line_shapes, stroke_color=stroke_color, id_offset=id_offset))\n",
    "            id_offset += len(line_shapes)\n",
    "        if len(fill_shapes) > 0:\n",
    "            shape_groups.append(self.create_fill_group(*fill_shapes, fill_color=fill_color, id_offset=id_offset))\n",
    "        return self.create_scene_from_groups(shapes, shape_groups)\n",
    "\n",
    "    def create_line_scene_from_points(self, *point_tensors, stroke_width=None, is_closed=False, expand_distance=False) -> Scene:\n",
    "        \"\"\"Shorthand for `create_line_scene` by passing `point_tensors` that\n",
    "           are converted to polygons.\"\"\"\n",
    "        return self.create_line_scene(*self.points_to_polygons(*point_tensors, stroke_width=stroke_width, \n",
    "                                                               is_closed=is_closed, expand_distance=expand_distance))\n",
    "\n",
    "    def points_to_polygons(self, *point_tensors, stroke_width=None, is_closed=False, expand_distance=False) -> List[Polygon]:\n",
    "        \"\"\"Convert `point_tensors` to a List of pydiffvg Polygons.\"\"\"\n",
    "        return [Polygon(points=self.expand_distance(pt) if expand_distance else pt,\n",
    "                        stroke_width=self.expand_stroke_width() if stroke_width is None else stroke_width,\n",
    "                        is_closed=is_closed) \\\n",
    "                for pt in point_tensors]\n",
    "\n",
    "    def render(self) -> Tensor:\n",
    "        \"\"\"Render `self.scenes` as a raster tensor using pydiffvg.\"\"\"\n",
    "        assert self.scenes is not None and len(self.scenes) == self.bs\n",
    "        cols = self.n_colors_out\n",
    "        output = torch.zeros(self.bs, cols, self.canvas_width, self.canvas_height) # .requires_grad_()\n",
    "        for i, s in enumerate(self.scenes):\n",
    "            raster = s.render(seed=self.fixed_seed)\n",
    "            if self.rendered_callback: \n",
    "                self.rendered_callback(raster=raster, batch_i=self.batch_i, item_i=i, scene=s, normalize=False, gamma=self.gamma)\n",
    "            if cols in (1, 3):\n",
    "                # Output is w,h,rgba, where with values in 0.-1. (and black thus 0., 0., 0., 1.)\n",
    "                # First, we apply alpha by mixing output with white in that proportion\n",
    "                if self.debug: self.stored.append(raster.clone())\n",
    "                alpha = raster[:, :, 3].unsqueeze(2).expand(-1, -1, 4)\n",
    "                white = torch.full_like(raster, 1.) * (1. - alpha)\n",
    "                raster = (white + raster * alpha)[:, :, :3] # Now raster is w,h,rgb\n",
    "                if cols == 1: # Convert to grayscale if needed\n",
    "                    raster *= RGBA_TO_GS # This is a crude NTSC sampling to grayscale\n",
    "                    raster = raster.sum(-1, keepdims=True)\n",
    "                raster = raster.permute(2, 0, 1) # Order channel-first\n",
    "            elif cols != 4: raise NotImplementedError(f\"n_colors_out '{cols}' can only be 1, 3 or 4.\")\n",
    "            raster = self.normalise_raster(raster)\n",
    "            if self.clip_raster: raster = raster.clip(0., 1.)\n",
    "            if self.apply_gamma: \n",
    "                if cols == 1: raster = raster.pow(1.0/self.gamma)\n",
    "                else: raster[:,:,:3] = raster[:,:,:3].pow(1.0/self.gamma)\n",
    "            # assert raster.requires_grad\n",
    "            output[i] = raster\n",
    "        return output\n",
    "\n",
    "add_docs(VectorRenderLayerBase)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class OCRLoss(CrossEntropyLossFlat):\n",
    "    \"\"\"Softmaxed CrossEntropyLossFlat between `ocr_model`'s prediction\n",
    "       and target category. Use after `VectorRenderLayerBase`.\"\"\"\n",
    "    stored: List[Tuple[float, int, float, Tensor]] = []\n",
    "    def __init__(self, ocr_model, debug = False, **kwargs):\n",
    "        super(OCRLoss, self).__init__(**kwargs)\n",
    "        assert ocr_model is not None\n",
    "        ocr_model.eval()\n",
    "        store_attr(\"ocr_model, debug\")\n",
    "\n",
    "    def __call__(self, inp, target):\n",
    "        pred = self.activation(self.ocr_model(inp))\n",
    "        loss = super(OCRLoss, self).__call__(pred, target)\n",
    "        if self.debug: self.stored.append((loss.item(), pred[0].argmax().item(), pred[0].max().item(), pred[0].detach()))\n",
    "        return loss\n",
    "\n",
    "def param_loss(x: Tensor, loss_start=4., loss_factor=1.) -> Tensor:\n",
    "    \"\"\"Calculate a linear loss for abs values above `loss_start` multiplied\n",
    "       by `loss_factor`.\"\"\"\n",
    "    return loss_factor * torch.maximum(x.abs() - loss_start, tensor(0.)).sum()\n",
    "\n",
    "class ParamLoss(Module):\n",
    "    \"\"\"Calculate a loss based on extreme parameter values.\"\"\"\n",
    "    def __init__(self, vector_model: VectorRenderLayerBase, loss_start=4., loss_factor=1., **kwargs):\n",
    "        super(ParamLoss, self).__init__(**kwargs)\n",
    "        assert vector_model is not None\n",
    "        store_attr(\"vector_model,loss_start,loss_factor\")\n",
    "\n",
    "    def forward(self, *args):\n",
    "        return param_loss(self.vector_model.params, self.loss_start, self.loss_factor)\n",
    "        \n",
    "class OCRAndParamLoss(Module):\n",
    "    \"\"\"Combined OCR and param loss.\"\"\"\n",
    "    def __init__(self, ocr_model, vector_model: VectorRenderLayerBase, loss_start=4., loss_factor=1., debug = False, **kwargs):\n",
    "        super(OCRAndParamLoss, self).__init__(**kwargs)\n",
    "        self.ocr_loss = OCRLoss(ocr_model=ocr_model, debug=debug, **kwargs)\n",
    "        self.param_loss = ParamLoss(vector_model=vector_model, loss_start=loss_start, loss_factor=loss_factor, **kwargs)\n",
    "\n",
    "    def forward(self, inp, target):\n",
    "        return self.ocr_loss(inp, target) + self.param_loss(inp, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class VectorLearner(Learner):\n",
    "    \"\"\"A simple extension to Learner offering some utility methods.\"\"\"\n",
    "    def __init__(self, image_saver=None, **kwargs):\n",
    "        super(VectorLearner, self).__init__(**kwargs)\n",
    "        store_attr(\"image_saver\")\n",
    "    \n",
    "    @property\n",
    "    def vocab(self) -> List[str]:\n",
    "        return self.dls.vocab\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        \"\"\"Shortcut for `self.model.reset_parameters`.\"\"\"\n",
    "        self.model.reset_parameters()\n",
    "\n",
    "    def set_param(self, *args, **kwargs):\n",
    "        \"\"\"Shortcut for `self.model.set_param`.\"\"\"\n",
    "        self.model.set_param(*args, **kwargs)\n",
    "\n",
    "    def render_letter(self, letter: str = \"A\", scale: float = None) -> PIL.Image:\n",
    "        \"\"\"Render a letter using the current vector model.\"\"\"\n",
    "        inp = tensor([self.vocab.index(letter)])\n",
    "        m = self.model\n",
    "        trn = m.training\n",
    "        m.eval()\n",
    "        with torch.no_grad(): img = m(inp).squeeze().clip(0., 1.) * 255\n",
    "        m.train(trn)\n",
    "        pil_img = PILImageBW.create(img).convert('RGB')\n",
    "        if scale is not None: pil_img = pil_img.resize((round(pil_img.width * scale), round(pil_img.height * scale)), resample=0)\n",
    "        return pil_img\n",
    "\n",
    "    def render_result_video(self, **kwargs):\n",
    "        \"\"\"Shortcut for `self.image_saver.render_result_video`\"\"\"\n",
    "        assert self.image_saver is not None\n",
    "        self.image_saver.render_result_video(**kwargs)\n",
    "\n",
    "    def calculate_losses(self, n = 20, param_ranges: List[Union[Tuple[float, float], float]] = None) -> pd.DataFrame:\n",
    "        \"\"\"Output loss statistics and predictions for different param values.\n",
    "           If `param_ranges` is supplied, it should contain the min and max\n",
    "           values to use for each parameter or a fixed value.\"\"\"\n",
    "        assert n > 1\n",
    "        assert self.loss_func.debug, \"Debug must be enabled for the loss function.\"\n",
    "        model = self.model\n",
    "        model.eval()\n",
    "        pl = model[0]\n",
    "        n_pars = pl.n_params\n",
    "        param_f = torch.full((n_pars,), 4)  if param_ranges is None else tensor([0. if type(x) is float else x[1] - x[0] for x in param_ranges])\n",
    "        param_c = torch.full((n_pars,), -2) if param_ranges is None else tensor([x  if type(x) is float else x[0] for x in param_ranges])\n",
    "        x,y = self.dls.one_batch()\n",
    "        stats = []\n",
    "        for i in range(n):\n",
    "            p_vals = param_c + param_f * i / (n - 1)\n",
    "            pl.params.data = p_vals\n",
    "            # Vector prediction\n",
    "            p = model(x)\n",
    "            _ = self.loss_func(p, y)\n",
    "            d = {\n",
    "                \"loss\": self.loss_func.stored[-1][0],\n",
    "                \"pred\": self.vocab[self.loss_func.stored[-1][1]],\n",
    "                \"pred_activation\": self.loss_func.stored[-1][2]\n",
    "                }\n",
    "            for j in range(n_pars): d[f\"param_{j}\"] = p_vals[j].item()\n",
    "            stats.append(d)\n",
    "        return pd.DataFrame(stats)\n",
    "\n",
    "add_docs(VectorLearner)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 01_fontlearnertests.ipynb.\n",
      "Converted 02_lettervectors.ipynb.\n",
      "Converted aifont_core.ipynb.\n",
      "Converted aifont_fontlearner.ipynb.\n",
      "Converted aifont_fontsampler.ipynb.\n",
      "Converted aifont_ocrlearner.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "from nbdev.export import notebook2script; notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5710b12fb88680bf60c169aecc91e9487b0350ee3a6536206b6750ffeed12b61"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('ai-font-p3.9')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
