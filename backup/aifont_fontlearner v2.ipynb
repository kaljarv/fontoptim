{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp fontlearner\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Font Learner\n",
    "\n",
    "> Diffvg-based learner for font optimisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from aifont.core import *\n",
    "from fastai.data.all import *\n",
    "from fastai.vision.all import *\n",
    "import ffmpeg\n",
    "from nbdev.showdoc import *\n",
    "import PIL\n",
    "import pydiffvg\n",
    "from typing import Callable, List, Protocol, Tuple, Union"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants and Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "DIFFVG_BLACK = tensor(0., 0., 0., 1.)\n",
    "EMPTY_TENSOR = tensor([])\n",
    "tensor(0.)  = tensor(0.)\n",
    "\n",
    "def get_vocab(dls_or_learn: Union[DataLoaders, Learner]) -> List[str]:\n",
    "    \"\"\"Utility for getting the vocab from a Learner or DataLoaders.\"\"\"\n",
    "    if isinstance(dls_or_learn, Learner): dls_or_learn = dls_or_learn.dls\n",
    "    vocab = dls_or_learn.vocab\n",
    "    if type(vocab) == L and type(vocab[0]) == list: \n",
    "        if vocab[0] != vocab[1]: warn(\"The two vocabs in dls do not match! Using the first one.\")\n",
    "        vocab = vocab[0]\n",
    "    return vocab\n",
    "\n",
    "class DebugCB(Callback):\n",
    "    \"\"\"A `Callback` for debugging a `FontParamLayer`.\"\"\"\n",
    "    d_vals = []\n",
    "    d_grads = []\n",
    "    w_vals = []\n",
    "    w_grads = []\n",
    "    def before_fit(self):\n",
    "        # self.model[0].distance_params.retain_grad()\n",
    "        pass\n",
    "    def before_step(self):\n",
    "        m = self.model[0]\n",
    "        self.d_vals.append(m.distance_params.clone())\n",
    "        self.d_grads.append(m.distance_params.grad.clone())\n",
    "        self.w_vals.append(m.width_params.clone())\n",
    "        self.w_grads.append(m.width_params.grad.clone())\n",
    "    def plot(self) -> None:\n",
    "        # val = self.model[0].distance_params[0].item()\n",
    "        # grad_df.iloc[(grad_df.Vals - val).abs().argmin()]\n",
    "        num_d = self.d_vals[0].size(0)\n",
    "        num_w = self.w_vals[0].size(0)\n",
    "        def _items(tensor_list, idx): return [x[idx].item() for x in tensor_list]\n",
    "        plt.figure(figsize=(10,10))\n",
    "        for i in range(num_d): plt.scatter(_items(self.d_vals, i), _items(self.d_grads, i), label=f\"Distance param {i}\")\n",
    "        for i in range(num_w): plt.scatter(_items(self.w_vals, i), _items(self.w_grads, i), label=f\"Width param {i}\")\n",
    "        plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rendering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class Scene:\n",
    "    \"\"\"Just a utility to hold the different scene arguments together.\"\"\"\n",
    "    last_seed: int = None\n",
    "    def __init__(self, shapes: list[any], shape_groups: list[pydiffvg.ShapeGroup], \n",
    "        canvas_width = 256, canvas_height = 256, samples = 2):\n",
    "        assert shapes is not None and len(shapes) != 0\n",
    "        assert shape_groups is not None and len(shape_groups) != 0\n",
    "        store_attr()\n",
    "\n",
    "    def get_scene_args(self) -> list:\n",
    "        \"\"\"Get the serialize scene for passing to `pydiffvg.RenderFunction`.\"\"\"\n",
    "        return pydiffvg.RenderFunction.serialize_scene(self.canvas_width, \n",
    "                                                       self.canvas_height, \n",
    "                                                       self.shapes, \n",
    "                                                       self.shape_groups)\n",
    "\n",
    "    def render(self, seed: int = None, do_render_grad = False) -> Tensor:\n",
    "        \"\"\"Render the scene using pydffiv `RenderFunction` or its\n",
    "           gradient if `do_render_grad`.\"\"\"\n",
    "        if seed is None: seed = random.randint(0, 1e6)\n",
    "        self.last_seed = seed\n",
    "        scene_args = self.get_scene_args()\n",
    "        w = self.canvas_width\n",
    "        h = self.canvas_height\n",
    "        s = self.samples\n",
    "        args = [w, h, s, s, seed, None] + scene_args\n",
    "        rf = pydiffvg.RenderFunction\n",
    "        return rf.render_grad(torch.ones(w, h, 4, device=pydiffvg.get_device()), *args) \\\n",
    "               if do_render_grad else rf.apply(*args)\n",
    "\n",
    "    def render_grad(self, seed: int = None) -> Tensor:\n",
    "        \"\"\"Render the gradient as raster.\"\"\"\n",
    "        return self.render(seed=self.last_seed, do_render_grad=True)\n",
    "    \n",
    "add_docs(Scene)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class ImageSaver:\n",
    "    \"\"\"Create a callback to pass to `VectorRenderLayer` as `rendered_callback`\n",
    "       to save rendered images and optionally grads.\"\"\"\n",
    "    canvas_height: int = None\n",
    "    canvas_width: int = None\n",
    "    iter_files: List[str] = []\n",
    "    grad_files: List[str] = []\n",
    "    def __init__(self, folder: str, save_grad = False, iter_name = \"iter\", \n",
    "                 grad_name = \"grad\"):\n",
    "        assert folder is not None\n",
    "        if folder.endswith(\"/\"):\n",
    "            folder = folder[:-1]\n",
    "        store_attr()\n",
    "\n",
    "    def __call__(self, raster: Tensor, batch_i: int, item_i: int, scene: Scene,\n",
    "                 normalize = False, gamma = 2.2) -> None:\n",
    "        suffix = f\"_{batch_i}_{item_i}\"\n",
    "        self.iter_files.append(self.save_image(raster, f\"{self.iter_name}{suffix}\", normalize=normalize, gamma=gamma))\n",
    "        if self.save_grad: \n",
    "            grad = scene.render_grad()\n",
    "            self.grad_files.append(self.save_image(grad, f\"{self.grad_name}{suffix}\"))\n",
    "\n",
    "    def save_image(self, raster: Tensor, filename: str, normalize = False, gamma = 2.2) -> Str:\n",
    "        \"\"\"Save the `raster` tensor as image file and return filename used.\"\"\"\n",
    "        if not self.canvas_height:\n",
    "            self.canvas_height = raster.size(0)\n",
    "            self.canvas_width  = raster.size(1)\n",
    "        fn = f\"{self.folder}/{filename}.png\"\n",
    "        pydiffvg.imwrite(raster.cpu(), fn, normalize=normalize, gamma=gamma)\n",
    "        return fn\n",
    "    \n",
    "    def render_result_video(self, delete_imgs = False, frame_rate=24, grad=False) -> None:\n",
    "        \"\"\"Render intermediate images as a video.\"\"\"\n",
    "        files = self.grad_files if grad else self.iter_files\n",
    "        out = os.path.join(self.folder, \"grads.mp4\" if grad else \"iters.mp4\")\n",
    "        frames = ffmpeg.input('pipe:', r=str(frame_rate))\n",
    "        process = ffmpeg.input(f\"color=c=white:s={self.canvas_width}x{self.canvas_height}\", f=\"lavfi\") \\\n",
    "                        .overlay(frames, eof_action=\"endall\") \\\n",
    "                        .output(out) \\\n",
    "                        .overwrite_output() \\\n",
    "                        .run_async(pipe_stdin=True, quiet=True)\n",
    "        for in_file in files:\n",
    "            with open(in_file, 'rb') as f: process.stdin.write(f.read())\n",
    "        # Close stdin pipe - FFmpeg fininsh encoding the output file.\n",
    "        process.stdin.close()\n",
    "        process.wait()\n",
    "        if delete_imgs: \n",
    "            for f in files: os.remove(f)\n",
    "        print(\"Rendering video done!\")\n",
    "\n",
    "add_docs(ImageSaver)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataLoader\n",
    "\n",
    "Custom DataLoader for getting letter classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class LetterDL(DataLoader):\n",
    "    \"\"\"A dummy data loader for use with font vector optimisation.\n",
    "       Pass the same `vocab` as in the OCR model. Batch size defaults\n",
    "       to `epoch_len`.\"\"\"\n",
    "    current_i = 0\n",
    "    def __init__(self, vocab: CategoryMap, letters: Tuple[str, ...] = (\"A\",), \n",
    "                 epoch_len = 10, bs = 1, **kwargs):\n",
    "        assert vocab is not None\n",
    "        super(LetterDL, self).__init__(bs=bs, n = epoch_len * bs, **kwargs)\n",
    "        self.categorizer = Categorize(vocab=vocab)\n",
    "        store_attr()\n",
    "\n",
    "    def create_item(self, s) -> Tuple[TensorCategory, TensorCategory]:\n",
    "        \"\"\"Return the CategoryTensor for a random letter from `letters`.\"\"\"\n",
    "        if self.current_i == self.n:\n",
    "            self.current_i = 0\n",
    "            stop()\n",
    "        self.current_i += 1\n",
    "        r = self.categorizer.encodes(random.choice(self.letters))\n",
    "        # Return x and y as a copy of x\n",
    "        return r, r.clone()\n",
    "\n",
    "add_docs(LetterDL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector Model\n",
    "\n",
    "The vector model consists of a `FontParamLayer`, which holds the parameters to optimise, and a subclass of `VectorRenderLayer`, which handles the creation of the letter vectors.\n",
    "\n",
    "> Note that the utility of this bisection is tentative, and the params might as well be contained within the `VectorRenderLayer`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Font Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class FontParamLayer(Module):\n",
    "    \"\"\"Creates `n_distance_params` params with tanh activation and \n",
    "       `n_width_params` with sigmoid activation and concats these \n",
    "       with inputs. Thus, the output is a tensor of size\n",
    "       `(bs, n_distance_params + n_width_params + 1)` and the param\n",
    "       ranges are [-1, 1] for `distance_params` and [0, 1] for\n",
    "       `width_params`, which should be mapped to to min and max\n",
    "       stroke widths by the next layer. `init_range`\n",
    "       defines the range of the parameter value space when\n",
    "       initialised at random by `reset_parameters` as a fraction\n",
    "       of the total range and centered around the middle.\n",
    "       NB. The separation of the parameters into there two types\n",
    "       may not be very reasonable. It's copied from `diffvg` with\n",
    "       little regard to any underlying motivation except that the\n",
    "       distance values are treated as offsets from the canvas \n",
    "       center.\"\"\"\n",
    "    sigmoid = torch.nn.Sigmoid()\n",
    "    tanh = torch.nn.Tanh()\n",
    "    weight = True # We need to include this for fastai to recognize this layer as trainable\n",
    "\n",
    "    def __init__(self, n_distance_params=1, n_width_params=1, seed=None, init_range=.5):\n",
    "        if seed is not None: torch.random.manual_seed(seed)\n",
    "        self.distance_params = torch.nn.Parameter(torch.empty(n_distance_params)) if n_distance_params else EMPTY_TENSOR\n",
    "        self.width_params =    torch.nn.Parameter(torch.empty(n_width_params)) if n_width_params else EMPTY_TENSOR\n",
    "        n_distance_params =    n_distance_params or 0\n",
    "        n_width_params =       n_width_params or 0\n",
    "        store_attr()\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"\\n\".join([\n",
    "            \"FontParamLayer\",\n",
    "            \"- distance params:\",\n",
    "            *[f\"  {x.item()} ({self.tanh(x).item()})\" for x in self.distance_params],\n",
    "            \"- width params:\",\n",
    "            *[f\"  {x.item()} ({self.sigmoid(x).item()})\" for x in self.width_params],\n",
    "            ])\n",
    "               \n",
    "    def reset_parameters(self):\n",
    "        \"\"\"Randomly init the parameters around the middle of possible values.\"\"\"\n",
    "        d = self.init_range * 2 # We approximate the domain as [-2, 2]\n",
    "        if self.n_distance_params: self.distance_params.data.uniform_(-d, d)\n",
    "        if self.n_width_params:    self.width_params.data.uniform_(-d, d)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Convert the input x to size (bs, 1) if it's one-dimensional and\n",
    "           concat the params before each batch.\"\"\"\n",
    "        if x.ndim == 1: x = x.unsqueeze(1)\n",
    "        elif x.ndim != 2: raise ValueError(\"Input can only be 1- or 2-dimensional.\")\n",
    "        params = torch.cat((self.tanh(self.distance_params), self.sigmoid(self.width_params)))\n",
    "        params = params.expand(x.size(0), -1)\n",
    "        return torch.column_stack((params, x))\n",
    "\n",
    "add_docs(FontParamLayer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector Rendering Layer Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "class Normaliser(Protocol):\n",
    "    \"\"\"For normalising rasters for the OCR.\"\"\"\n",
    "    mean: float\n",
    "    std: float\n",
    "\n",
    "class VectorRenderLayerBase(Module):\n",
    "    \"\"\"Base for vector render layers. Get's input from a FontParamLayer\n",
    "       and returns the diffvg rendering. Override `create_scenes` in\n",
    "       subclasses and save the results in `self.scenes` of which there\n",
    "       should be `bs`. `forward` calls `render` which renders the ch\n",
    "       scenes and permutes to match the OCR model. Note that the workflow\n",
    "       is based on greyscale images and we're only using the alpha value\n",
    "       of the diffvg render output. Init parameters:\n",
    "       `canvas_width`, `canvas_height`: rastered canvas dims\n",
    "       `raster_norm`: use the normaliser from the OCR `dls`\n",
    "       `clip_raster`: whether to clip color values to [0., 1.] as is done when\n",
    "            saving image (note that the values produced by the render\n",
    "            function value wildly up to more than 10. so setting this to\n",
    "            False is advised against)\n",
    "       `apply_gamma`: whether to apply `gamma` to the color values similarly\n",
    "            to clipping above\n",
    "       `n_distance_params`: number of params with tanh activation\n",
    "       `n_width_params`: number of params with sigmoid activation\n",
    "       `eps`: amount of random jitter added to `distance_params` \n",
    "       `n_colors_out`: color channels out\n",
    "       `max_distance`: the maximum fraction [0., 1.] of canvas dims \n",
    "            `distance_params` can span\n",
    "       `fixed_seed`: fixed seed value to pass to `pydiffvg.RenderFunction`\n",
    "       `gamma`: set to override default gamma of 2.2 for colour images and 1.\n",
    "            for grayscale ones\n",
    "       `stroke_width`: stroke width for shape generator helpers\n",
    "            (note that this is defined as a fraction of `canvas_size`);\n",
    "            either a float or a tuple of min and max width and used by\n",
    "            `expand_stroke_width`\n",
    "       `stroke_color`: default stroke color for shape generator helpers\n",
    "       `rendered_callback`: set to an ImageSaver to save interim renders\"\"\"\n",
    "    batch_i = -1\n",
    "    bs: int = None\n",
    "    eps_tensor: Tensor = None\n",
    "    scenes: List[Scene] = []\n",
    "    x: Tensor = None\n",
    "    def __init__(self, canvas_width: int, canvas_height: int, raster_norm: Normaliser = None,\n",
    "                 n_distance_params = 1, n_width_params = 1, eps = None, clip_raster = True,\n",
    "                 apply_gamma = True, n_colors_out = 1, max_distance = 1., fixed_seed: int = None, \n",
    "                 gamma: float = None, stroke_width: Union[float, Tuple[float, float]] = 1./28, \n",
    "                 stroke_color = DIFFVG_BLACK, \n",
    "                 rendered_callback: Callable[[Tensor, int, int, Scene, bool, float], None] = None):\n",
    "        super(VectorRenderLayerBase, self).__init__()\n",
    "        self.canvas_size = max(canvas_width, canvas_height)\n",
    "        stroke_width = tensor(stroke_width)\n",
    "        if canvas_width != canvas_height: \n",
    "            warn(f\"When canvas is not square ({canvas_width}x{canvas_height}), \"\n",
    "                  \"some dimensions may be expanded outside it.\")\n",
    "        n_width_params = n_width_params or 0\n",
    "        if gamma is None: gamma = 1. if n_colors_out == 1 else 2.2\n",
    "        store_attr()\n",
    "\n",
    "    def get_item(self, i: int) -> Tensor:\n",
    "        \"\"\"Get item `i` in the batch `x`\"\"\"\n",
    "        return self.x[i]\n",
    "\n",
    "    def get_distance_params(self, i: int) -> Tensor:\n",
    "        \"\"\"`distance_params` for item `i` in the batch passed by \n",
    "           `FontParamLayer` as part of `x`\"\"\"\n",
    "        return self.get_item(i)[:self.n_distance_params]\n",
    "\n",
    "    def get_width_params(self, i: int) -> Tensor:\n",
    "        \"\"\"`width_params` for item `i` in the batch passed by \n",
    "           `FontParamLayer` as part of `x`\"\"\"\n",
    "        return self.get_item(i)[self.n_distance_params : self.n_distance_params + self.n_width_params]\n",
    "\n",
    "    def get_inputs(self, i: int) -> Tensor:\n",
    "        \"\"\"`bs` number of letter categories for item `i` in the batch passed \n",
    "            by `FontParamLayer` as part of `x`\"\"\"\n",
    "        return self.get_item(i)[self.n_distance_params + self.n_width_params :]\n",
    "\n",
    "    @property\n",
    "    def eps(self) -> float:\n",
    "        \"\"\"Get the amount of random noise to add when evaluating params.\"\"\"\n",
    "        return self._eps\n",
    "\n",
    "    @eps.setter\n",
    "    def eps(self, value: float):\n",
    "        \"\"\"Set the amount of random noise to add when evaluating params.\"\"\"\n",
    "        self._eps = value\n",
    "        self.eps_tensor = None\n",
    "\n",
    "    def add_eps(self) -> None:\n",
    "        \"\"\"Apply random eps to distance params. Cf. diffvg/apps/generative \n",
    "           modeling/rendering.render_lines\"\"\"\n",
    "        assert self.bs is not None\n",
    "        if not self.eps or not self.n_distance_params: return\n",
    "        if self.eps_tensor is None:\n",
    "            # Premake a tensor that has eps values matching distance_params\n",
    "            # in the input, i.e. the first items on each row, and zeros elsewhere\n",
    "            self.eps_tensor = torch.column_stack((torch.full((self.bs, self.n_distance_params), self.eps),\n",
    "                                                  torch.zeros(self.bs, self.x.size(1) - self.n_distance_params)))\n",
    "        self.x = self.x + self.eps_tensor * torch.randn_like(self.x)\n",
    "\n",
    "    def expand_distance(self, vals: Tensor) -> Tensor:\n",
    "        \"\"\"Expand values based on `[-m, m]` where `m = self.max_distance` \n",
    "           central coordinates.\"\"\"\n",
    "        return (.5 * (vals + 1.) * self.max_distance + (1 - self.max_distance) / 2) * self.canvas_size\n",
    "\n",
    "    def expand_stroke_width(self, vals: Tensor = None) -> Tensor:\n",
    "        \"\"\"Expand `vals`,  based on `[min_stroke_width, max_stroke_width] * canvas_height`.\"\"\"\n",
    "        if vals is None: vals = tensor(1.)\n",
    "        w = vals * self.stroke_width if self.stroke_width.ndim == 0 \\\n",
    "            else self.stroke_width[0] + vals * (self.stroke_width[1] - self.stroke_width[0])\n",
    "        return w * self.canvas_size\n",
    "\n",
    "    def normalise_raster(self, raster: Tensor) -> Tensor:\n",
    "        \"\"\"Apply normalisation to `raster`. Not useful for grayscale letters.\"\"\"\n",
    "        if not self.raster_norm: return raster\n",
    "        return (raster - self.raster_norm.mean) / self.raster_norm.std \n",
    "\n",
    "    def forward(self, x) -> Tensor:\n",
    "        \"\"\"Render letters defined in `x`, which also contains the font parameters.\"\"\"\n",
    "        self.batch_i += 1\n",
    "        self.x = x\n",
    "        self.bs = x.size(0)\n",
    "        self.add_eps()\n",
    "        self.scenes = [None] * self.bs\n",
    "        self.create_scenes()\n",
    "        return self.render()\n",
    "\n",
    "    def create_scenes(self) -> None:\n",
    "        \"\"\"Override this in subclasses to create the vector scenes for the letters.\"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def create_line_scene(self, *shapes) -> Scene:\n",
    "        \"\"\"Create a simple line-drawing Scene with shapes.\"\"\"\n",
    "        shape_groups = [pydiffvg.ShapeGroup(shape_ids=tensor(list(range(len(shapes)))),\n",
    "                                            fill_color=None,\n",
    "                                            stroke_color=self.stroke_color,\n",
    "                                            use_even_odd_rule=False)]\n",
    "        return Scene(shapes=shapes, shape_groups=shape_groups, \n",
    "                     canvas_width = self.canvas_width, canvas_height = self.canvas_height)\n",
    "\n",
    "    def create_line_scene_from_points(self, *point_tensors, stroke_width=None) -> Scene:\n",
    "        \"\"\"Shorthand for `create_line_scene` by passing `point_tensors` that\n",
    "           are converted to polygons.\"\"\"\n",
    "        return self.create_line_scene(*self.points_to_polygons(*point_tensors, stroke_width=stroke_width))\n",
    "\n",
    "    def points_to_polygons(self, *point_tensors, stroke_width=None) -> List[pydiffvg.Polygon]:\n",
    "        \"\"\"Convert `point_tensors` to a List of pydiffvg Polygons.\"\"\"\n",
    "        return [pydiffvg.Polygon(points=pt, \n",
    "                                 stroke_width=self.expand_stroke_width() if stroke_width is None else stroke_width,\n",
    "                                 is_closed=False) \\\n",
    "                for pt in point_tensors]\n",
    "\n",
    "\n",
    "    def render(self) -> Tensor:\n",
    "        \"\"\"Render `self.scenes` as a raster tensor using pydiffvg.\"\"\"\n",
    "        assert self.scenes is not None and len(self.scenes) == self.bs\n",
    "        cols = self.n_colors_out\n",
    "        output = torch.zeros(self.bs, cols, self.canvas_width, self.canvas_height) # .requires_grad_()\n",
    "        for i, s in enumerate(self.scenes):\n",
    "            raster = s.render(seed=self.fixed_seed)\n",
    "            if self.rendered_callback: \n",
    "                self.rendered_callback(raster=raster, batch_i=self.batch_i, item_i=i, scene=s, normalize=False, gamma=self.gamma)\n",
    "            if cols in (1, 3):\n",
    "                raster = raster[:,:,-1]               # w,h; float 0.-1. where 1. is black\n",
    "                raster = 1. - raster                  # w,h; float 0.-1. where 0. is black\n",
    "                raster = raster.expand(cols, -1, -1)  # c,w,h; all channels equal\n",
    "            elif cols != 4: raise NotImplementedError(f\"n_colors_out '{cols}' can only be 1, 3 or 4.\")\n",
    "            raster = self.normalise_raster(raster)\n",
    "            if self.clip_raster: raster = raster.clip(0., 1.)\n",
    "            if self.apply_gamma: \n",
    "                if cols == 1: raster = raster.pow(1.0/self.gamma)\n",
    "                else: raster[:,:,:3] = raster[:,:,:3].pow(1.0/self.gamma)\n",
    "            # assert raster.requires_grad\n",
    "            output[i] = raster\n",
    "        return output\n",
    "\n",
    "add_docs(VectorRenderLayerBase)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class OCRLoss(CrossEntropyLossFlat):\n",
    "    \"\"\"Softmaxed CrossEntropyLossFlat between `ocr_model`'s prediction\n",
    "       and target category. Use after `VectorRenderLayerBase`.\"\"\"\n",
    "    stored: List[Tuple[float, int, float, Tensor]] = []\n",
    "    def __init__(self, ocr_model, debug = False, **kwargs):\n",
    "        super(OCRLoss, self).__init__(**kwargs)\n",
    "        assert ocr_model is not None\n",
    "        ocr_model.eval()\n",
    "        store_attr(\"ocr_model, debug\")\n",
    "\n",
    "    def __call__(self, inp, target):\n",
    "        pred = self.activation(self.ocr_model(inp))\n",
    "        loss = super(OCRLoss, self).__call__(pred, target)\n",
    "        if self.debug: self.stored.append((loss.item(), pred[0].argmax().item(), pred[0].max().item(), pred[0].detach()))\n",
    "        return loss\n",
    "\n",
    "def param_loss(x: Tensor, loss_start=1.5, loss_factor=1.) -> Tensor:\n",
    "    \"\"\"Calculate a linear loss for abs values above `loss_start` multiplied\n",
    "       by `loss_factor`.\"\"\"\n",
    "    return loss_factor * torch.maximum(x.abs() - loss_start, tensor(0.)).sum()\n",
    "\n",
    "class ParamLoss(Module):\n",
    "    \"\"\"Calculate a loss based on extreme parameter values.\"\"\"\n",
    "    def __init__(self, param_layer, loss_start_dist=1.5, loss_start_width=4., loss_factor=1., **kwargs):\n",
    "        super(ParamLoss, self).__init__(**kwargs)\n",
    "        assert param_layer is not None\n",
    "        store_attr(\"param_layer,loss_start_dist,loss_start_width,loss_factor\")\n",
    "\n",
    "    def forward(self, *args):\n",
    "        return param_loss(self.param_layer.distance_params, self.loss_start_dist, self.loss_factor) + \\\n",
    "               param_loss(self.param_layer.width_params, self.loss_start_width, self.loss_factor)\n",
    "        \n",
    "class OCRAndParamLoss(Module):\n",
    "    \"\"\"Combined OCR and param loss.\"\"\"\n",
    "    def __init__(self, ocr_model, param_layer, loss_start_dist=1.5, loss_start_width=4., \n",
    "                 loss_factor=1., debug = False, **kwargs):\n",
    "        super(OCRAndParamLoss, self).__init__(**kwargs)\n",
    "        self.ocr_loss = OCRLoss(ocr_model=ocr_model, debug=debug, **kwargs)\n",
    "        self.param_loss = ParamLoss(param_layer=param_layer, loss_start_dist=loss_start_dist, \n",
    "                                    loss_start_width=loss_start_width, loss_factor=loss_factor, **kwargs)\n",
    "\n",
    "    def forward(self, inp, target):\n",
    "        return self.ocr_loss(inp, target) + self.param_loss(inp, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class VectorLearner(Learner):\n",
    "    \"\"\"A simple extension to Learner offering some utility methods.\"\"\"\n",
    "    def __init__(self, image_saver=None, **kwargs):\n",
    "        super(VectorLearner, self).__init__(**kwargs)\n",
    "        store_attr(\"image_saver\")\n",
    "    \n",
    "    @property\n",
    "    def vocab(self) -> List[str]:\n",
    "        return self.dls.vocab\n",
    "\n",
    "    def render_letter(self, letter: str = \"A\") -> PIL.Image:\n",
    "        \"\"\"Render a letter using the current vector model.\"\"\"\n",
    "        inp = tensor([self.vocab.index(letter)])\n",
    "        m = self.model\n",
    "        trn = m.training\n",
    "        m.eval()\n",
    "        with torch.no_grad(): img = m(inp).squeeze().clip(0., 1.) * 255\n",
    "        m.train(trn)\n",
    "        return PILImageBW.create(img).convert('RGB')\n",
    "\n",
    "    def render_result_video(self, **kwargs):\n",
    "        \"\"\"Shortcut for `self.image_saver.render_result_video`\"\"\"\n",
    "        assert self.image_saver is not None\n",
    "        self.image_saver.render_result_video(**kwargs)\n",
    "\n",
    "    def calculate_losses(self, n = 20, param_ranges: List[Union[Tuple[float, float], float]] = None) -> pd.DataFrame:\n",
    "        \"\"\"Output loss statistics and predictions for different param values.\n",
    "           If `param_ranges` is supplied, it should contain the min and max\n",
    "           values to use for each parameter or a fixed value.\"\"\"\n",
    "        assert n > 1\n",
    "        assert self.loss_func.debug, \"Debug must be enabled for the loss function.\"\n",
    "        model = self.model\n",
    "        model.eval()\n",
    "        pl = model[0]\n",
    "        n_dp = pl.n_distance_params\n",
    "        n_wp = pl.n_width_params\n",
    "        n_pars = n_dp + n_wp \n",
    "        param_f = torch.full((n_pars,), 4)  if param_ranges is None else tensor([0. if type(x) is float else x[1] - x[0] for x in param_ranges])\n",
    "        param_c = torch.full((n_pars,), -2) if param_ranges is None else tensor([x  if type(x) is float else x[0] for x in param_ranges])\n",
    "        x,y = self.dls.one_batch()\n",
    "        stats = []\n",
    "        for i in range(n):\n",
    "            p_vals = param_c + param_f * i / (n - 1)\n",
    "            if n_dp: pl.distance_params.data = p_vals[:n_dp]\n",
    "            if n_wp: pl.width_params.data = p_vals[n_dp : n_dp + n_wp]\n",
    "            # Vector prediction\n",
    "            p = model(x)\n",
    "            _ = self.loss_func(p, y)\n",
    "            d = {\n",
    "                \"loss\": self.loss_func.stored[-1][0],\n",
    "                \"pred\": self.vocab[self.loss_func.stored[-1][1]],\n",
    "                \"pred_activation\": self.loss_func.stored[-1][2]\n",
    "                }\n",
    "            for j in range(n_dp): d[f\"distance_param_{j}\"] = p_vals[j].item()\n",
    "            for j in range(n_wp): d[f\"width_param_{j}\"] = p_vals[n_dp + j].item()\n",
    "            stats.append(d)\n",
    "        return pd.DataFrame(stats)\n",
    "\n",
    "add_docs(VectorLearner)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 01_fontlearnertests.ipynb.\n",
      "Converted 02_lettervectors.ipynb.\n",
      "Converted aifont_core.ipynb.\n",
      "Converted aifont_fontlearner.ipynb.\n",
      "Converted aifont_fontsampler.ipynb.\n",
      "Converted aifont_ocrlearner.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "from nbdev.export import notebook2script; notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5710b12fb88680bf60c169aecc91e9487b0350ee3a6536206b6750ffeed12b61"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('ai-font-p3.9')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
